[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2021-04-23-FairnessReport.html",
    "href": "posts/2021-04-23-FairnessReport.html",
    "title": "Fairness in Hiring and Salary Statistical Report ‚öñÔ∏è",
    "section": "",
    "text": "An analysis of fictional company Black Saber - toc: true - branch: master - badges: true - comments: true - author: Jaekang Lee - categories: [fastpages, r, glmer, data_exploration, data wrangling, report]"
  },
  {
    "objectID": "posts/2021-04-23-FairnessReport.html#full-report",
    "href": "posts/2021-04-23-FairnessReport.html#full-report",
    "title": "Fairness in Hiring and Salary Statistical Report ‚öñÔ∏è",
    "section": "Full Report",
    "text": "Full Report\nhttps://github.com/leejaeka/black_saber_statistic_reporting\n\nHere are some sneak peaks!\n\n#hide_input\nfrom fastbook import *\nfrom fastai.vision.widgets import *\none = PILImage.create(\"images/black_saber/1.png\")\ntwo = PILImage.create(\"images/black_saber/2.png\")\nthree = PILImage.create(\"images/black_saber/3.png\")\nfour = PILImage.create(\"images/black_saber/4.png\")\nfive = PILImage.create(\"images/black_saber/5.png\")\n\n\ndisplay(one.to_thumb(800,700))\ndisplay(two.to_thumb(800,700))\ndisplay(three.to_thumb(800,700))\ndisplay(four.to_thumb(800,700))\ndisplay(five.to_thumb(800,700))"
  },
  {
    "objectID": "posts/2021-01-25-jane-predict.html",
    "href": "posts/2021-01-25-jane-predict.html",
    "title": "Jane Street Market Prediction üéØ",
    "section": "",
    "text": "Jane Street Market Prediction Kaggle Competition\nGot a score of 9443.499 (249th place out of 3616 competitors) using MLP."
  },
  {
    "objectID": "posts/2021-01-25-jane-predict.html#methodology",
    "href": "posts/2021-01-25-jane-predict.html#methodology",
    "title": "Jane Street Market Prediction üéØ",
    "section": "Methodology",
    "text": "Methodology\n\nNull Values üà≥\nAs discussed before in my EDA notebook, we have couple of options to handle null values.  1. Drop all nans 2. Impute with median or mean 3. Feedforward/backward 4. KNN imputer 5. Be creative! \nIn this notebook, I used KNN imputer with 5 nearest neighbors to fill the nans. This takes a long time to run so I suggest downloading the imputed data files from here by louise2001. Note that he also uploaded soft and iterative imputes.\n\n#hide_input\ndf = df.set_index('ts_id', drop=True)\ndf.drop(columns=[f\"resp_{i}\" for i in range(1, 5)], inplace=True)\nprint(f'Done loading data. df shape is {df.shape}')\nTARGET = 'resp'\nFEATURES = [f\"feature_{i}\" for i in range(1, 130)]\ntrain_pos, train_neg = df.loc[df.feature_0 &gt; 0], df.loc[df.feature_0 &lt; 0]\ntrain_pos.drop(columns=[TARGET, 'feature_0'], inplace=True)\ntrain_neg.drop(columns=[TARGET, 'feature_0'], inplace=True)\ngc.collect()\nnan_neg = pd.read_csv(\"../input/janestreetimputeddata/nan_neg.csv\", header=None, sep=' ').values.astype(int)\nnan_pos = pd.read_csv(\"../input/janestreetimputeddata/nan_pos.csv\", header=None, sep=' ').values.astype(int)\n\n# Split into X and y\nfrom copy import deepcopy as dc\nX_pos = dc(train_pos[FEATURES].values)\nX_neg = dc(train_neg[FEATURES].values)\ndel train_pos, train_neg\ngc.collect()\n\n# load files \nfile = 'knn_5'\npath = \"../input/janestreetimputeddata/\"\nX_pos[nan_pos[0], nan_pos[1]] = pd.read_csv(path+f\"positive_{file}.csv\", \n                                            header=None, sep=' ').values.flatten()\nX_neg[nan_neg[0], nan_neg[1]] = pd.read_csv(path+f\"negative_{file}.csv\",                                         \n                                            header=None, sep=' ').values.flatten()\n\ndf = np.concatenate((X_pos, X_neg), axis=0)\ndel X_pos, X_neg, nan_neg, nan_pos\ngc.collect()\n\ndf = pd.DataFrame(df, columns = FEATURES)\n\n\n#hide_input\ntrain = dt.fread('../input/jane-street-market-prediction/train.csv')\ntrain = train.to_pandas()\ntrain = train[['date', 'weight', 'ts_id', 'resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4','feature_0']]\ngc.collect()\n\n# split train into 1s and 0s \nupper = train[train['feature_0'] == 1].sort_values(by='ts_id', axis=0, ascending=True)\nlower = train[train['feature_0'] == -1].sort_values(by='ts_id', axis=0, ascending=True)\n\n# attach\ntrain = pd.concat([upper, lower], axis = 0)\ndel upper, lower\ngc.collect()\n\n# save files\ndf.to_csv('imputed.csv', index=False)\ntrain.to_csv('one_on_top.csv', index=False)\ngc.collect()\n\n\n\nImport Data üìö\nIn this notebook, we are just going to load the imputed data instead of running the feature engineering here. Since it is very time consuming and takes a lot of RAM.\n\n#hide_output\nimputed_df = dt.fread('../input/data-wrangling/imputed.csv')\nimputed_df = imputed_df.to_pandas()\ntrain = dt.fread('../input/data-wrangling/one_on_top.csv')\ntrain = train.to_pandas()\n\ndf = pd.concat([train, imputed_df], axis=1, ignore_index=False)\ndel train, imputed_df\ngc.collect()\n\n3815\n\n\n\n#hide_input\ndf.head(5)\n\n\n\n\n\n\n\n\ndate\nweight\nts_id\nresp_1\nresp_2\nresp_3\nresp\nresp_4\nfeature_0\nfeature_1\n...\nfeature_120\nfeature_121\nfeature_122\nfeature_123\nfeature_124\nfeature_125\nfeature_126\nfeature_127\nfeature_128\nfeature_129\n\n\n\n\n0\n0\n0.000000\n0\n0.009916\n0.014079\n0.008773\n0.006270\n0.001390\n1\n-1.872746\n...\n0.603878\n6.086305\n1.168391\n8.313583\n1.782433\n14.018213\n2.653056\n12.600292\n2.301488\n11.445807\n\n\n1\n0\n0.138531\n4\n0.001252\n0.002165\n-0.001215\n-0.002604\n-0.006219\n1\n-3.172026\n...\n0.745019\n5.354213\n0.344850\n4.101145\n0.614252\n6.623456\n0.800129\n5.233243\n0.362636\n3.926633\n\n\n2\n0\n0.116557\n8\n-0.005460\n-0.007301\n-0.009085\n-0.001677\n-0.003546\n1\n-3.172026\n...\n1.120067\n4.167835\n1.537913\n4.785838\n1.637435\n6.968002\n2.354338\n5.825499\n1.778029\n4.740577\n\n\n3\n0\n0.160117\n9\n0.005976\n0.004345\n0.023712\n0.020317\n0.035360\n1\n2.744408\n...\n1.430190\n3.332330\n1.796860\n3.177064\n0.999252\n2.906432\n1.589816\n2.435999\n1.472419\n2.245991\n\n\n4\n0\n0.109651\n10\n0.006899\n0.003405\n0.000134\n-0.000690\n-0.003040\n1\n-3.172026\n...\n1.581096\n6.305170\n2.324290\n4.881133\n2.115830\n6.337250\n3.059392\n5.350729\n2.755876\n4.968388\n\n\n\n\n5 rows √ó 138 columns\n\n\n\n\n\nFeature Engineering üîß\nWe first do two feature engineering right off the bat. 1. We are going to drop any rows with ‚Äòweight‚Äô column equal to 0. This tells us that overall gain from such trade is 0. This would be like telling machine to just guess if learned correctly.  2. To explain why we are dropping all dates before day 85 can be shown visually below. Before the day 85, we can clearly see that the trend has changed quite drastically.\n\n#hide_input\ndf['weight_resp']   = df['weight']*df['resp']\ndf['weight_resp_1'] = df['weight']*df['resp_1']\ndf['weight_resp_2'] = df['weight']*df['resp_2']\ndf['weight_resp_3'] = df['weight']*df['resp_3']\ndf['weight_resp_4'] = df['weight']*df['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+(df.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+(df.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+(df.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+(df.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+(df.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return for resp _, 1, 2, 3, 4 (500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nplt.legend(loc=\"lower left\");\n\ndel df['weight_resp'], df['weight_resp_1'], df['weight_resp_2'],df['weight_resp_3'],df['weight_resp_4']\ndel resp, resp_1, resp_2, resp_3, resp_4\ngc.collect()\n\n7310\n\n\n\n\n\n\n\n\n\n\ndf = df.query('date &gt; 85').reset_index(drop = True) \ndf = df[df['weight'] != 0]\n\nNote that we only have 130 features compared to over 2 million datas. We easily make more features and avoid curse of dimensionality.\n\n#hide_output\n# Add action column (this is our target)\ndf['action'] = ((df['resp'].values) &gt; 0).astype(int)\n\n# feature names\nfeatures = [c for c in df.columns if \"feature\" in c]\n# resp names\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\n(1571415, 139)\n\n\n\n# We don't need time, date and weight anymore\ndf = df.loc[:, df.columns.str.contains('feature|resp', regex=True)]\n\nLet us do log transform and add them as new columns to the dataframe. Since performing on all features will give me out of memory error, let‚Äôs do this on group_0 which has tag_0 from features.csv. For more information, check out my EDA notebook.\n\n# Get log transformation for tag groups\ntag_0_group = [9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, 123]\nfor col in tag_0_group:\n    df[str('log_'+str(col))] = (df[str('feature_'+str(col))]-df[str('feature_'+str(col))].min()+1).transform(np.log)\n\n\n#hide_input\ndf.head(5)\n\n\n\n\n\n\n\n\nfeature_0\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\nfeature_7\nfeature_8\nfeature_9\n...\nlog_73\nlog_79\nlog_85\nlog_91\nlog_97\nlog_103\nlog_109\nlog_115\nlog_122\nlog_123\n\n\n\n\n0\n1\n3.151305\n5.467693\n-0.164505\n-0.189219\n0.663966\n0.988896\n0.661407\n0.897346\n2.184804\n...\n4.371497\n4.954968\n1.009198e-07\n1.235292e-07\n1.372731e+00\n7.735990e-01\n1.583237\n0.994426\n2.206237\n2.390646\n\n\n2\n1\n1.514607\n0.596214\n0.324062\n0.154730\n0.845069\n0.521491\n0.860309\n0.595352\n0.310387\n...\n4.385074\n4.956836\n1.009198e-07\n1.235292e-07\n7.875868e-01\n5.235099e-01\n0.793093\n0.487668\n2.191892\n2.100277\n\n\n4\n1\n-0.833827\n-0.049648\n0.262484\n0.421901\n0.098124\n0.171741\n0.034455\n0.169169\n0.512029\n...\n4.373749\n4.934122\n6.493667e-01\n8.441718e-01\n1.314139e+00\n1.969321e+00\n1.542457\n2.065858\n1.813171\n2.373700\n\n\n5\n1\n-3.172026\n-3.093182\n0.155047\n0.343024\n0.451619\n0.914937\n-0.596771\n-0.827370\n-0.974472\n...\n4.395633\n4.958532\n1.072512e+00\n7.936777e-01\n1.070915e-07\n7.520313e-08\n1.298665\n0.488986\n1.943198\n2.112894\n\n\n6\n1\n-3.172026\n-3.093182\n0.188790\n0.232964\n0.500087\n0.639725\n-0.083674\n0.019814\n-4.050318\n...\n4.390247\n4.959537\n8.272507e-01\n1.036085e+00\n6.587185e-01\n4.546515e-01\n1.000972\n1.028346\n1.824567\n2.101414\n\n\n\n\n5 rows √ó 147 columns\n\n\n\nOther ideas for feature engineering: 1. aggregating categorical columns by ‚Äòtags‚Äô on features.csv 2. count above mean, mean abs change, abs energy 3. log transform, kurt transform and other transforms 4. get creative!\nReasons not to do more feature engineering: 1. We have no idea what the features represent so it might be meaningless and dangerous 2. The dataset is really big so adding couple more columns will make me run out of memory 3. Much slower computation\n\n\nSplit data ‚úÇÔ∏è\nWe are going to use approximately 20000 data as test set. Our target value is action which we already have defined as any weight times resp above 0.(positive trades)\n\n#hide_output\n# Train test split\nfrom sklearn.model_selection import train_test_split\nX = df.loc[:, df.columns.str.contains('feature|log')]\ny = np.stack([(df[c] &gt; 0).astype('int') for c in resp_cols]).T\ndel df\ngc.collect()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42, shuffle=True)\n\n\ndel X, y\ngc.collect()\n\n0\n\n\n\n\nImplementation #2\n\nAlgoritms & Technique\nFor technique, we already applied a lot of our knowledge from our EDA into our dataset. (Feature engineering, imputing nulls, dropping &lt; 85 days, etc). For algorithm, we are going to use machine learning.  Now we have our data ready for training. There are hundreds of classifier model we can choose from and explore. However, after studying the Kaggle notebooks other participants have submitted, all high scored model seem to use Neural Network. I am going to try using random forest classifier and MLP to experiment here. Random Forest are always good for early because it is easy to just build and evaluate. Neural network is good at learning complicated models with the right parameter tuning.  #### Metrics Since this is a multiclass-classifying problem (5 types of ‚Äòresp‚Äô -&gt; gave us 5 pos vs neg target variables), for performance metrics we are going to use AUC(area under curve) as well as pure accuracy score for overall performance. With this metrics, we can see how our model is performing on unseen data and prevent overfitting easily to see any area for improvement accordingly. Sklearn and Seaborn provides great graphing tools for these metrics as well. #### Complications Note that the worst complication I had to face going through rest of this notebook was the size of the data. Depending on your computer‚Äôs RAM size and GPU computation speed this experience will vary. In my case, I ran into out of memory a hundreds of times. To avoid this, try using cloud training. If not make sure to save your computed data frequently and clean RAM with gc.collect and del function to free up space as much as possible."
  },
  {
    "objectID": "posts/2021-01-25-jane-predict.html#results",
    "href": "posts/2021-01-25-jane-predict.html#results",
    "title": "Jane Street Market Prediction üéØ",
    "section": "Results",
    "text": "Results\n\nRandom Forest Classifier evaluation and validation\n\n#hide_output\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=32, n_jobs=-1, verbose=2)\nrnd_clf.fit(X_train, y_train)\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n\n\nbuilding tree 1 of 100\nbuilding tree 2 of 100\nbuilding tree 3 of 100\nbuilding tree 4 of 100\nbuilding tree 5 of 100\nbuilding tree 6 of 100\nbuilding tree 7 of 100\nbuilding tree 8 of 100\nbuilding tree 9 of 100\nbuilding tree 10 of 100\nbuilding tree 11 of 100\nbuilding tree 12 of 100\nbuilding tree 13 of 100\nbuilding tree 14 of 100\nbuilding tree 15 of 100\nbuilding tree 16 of 100\nbuilding tree 17 of 100\nbuilding tree 18 of 100\nbuilding tree 19 of 100\nbuilding tree 20 of 100\nbuilding tree 21 of 100\nbuilding tree 22 of 100\nbuilding tree 23 of 100\nbuilding tree 24 of 100\nbuilding tree 25 of 100\nbuilding tree 26 of 100\nbuilding tree 27 of 100\nbuilding tree 28 of 100\nbuilding tree 29 of 100\nbuilding tree 30 of 100\nbuilding tree 31 of 100\nbuilding tree 32 of 100\nbuilding tree 33 of 100\nbuilding tree 34 of 100\nbuilding tree 35 of 100\nbuilding tree 36 of 100\nbuilding tree 37 of 100\nbuilding tree 38 of 100\n\n\n[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  9.8min\n\n\nbuilding tree 39 of 100\nbuilding tree 40 of 100\nbuilding tree 41 of 100\nbuilding tree 42 of 100\nbuilding tree 43 of 100\nbuilding tree 44 of 100\nbuilding tree 45 of 100\nbuilding tree 46 of 100\nbuilding tree 47 of 100\nbuilding tree 48 of 100\nbuilding tree 49 of 100\nbuilding tree 50 of 100\nbuilding tree 51 of 100\nbuilding tree 52 of 100\nbuilding tree 53 of 100\nbuilding tree 54 of 100\nbuilding tree 55 of 100\nbuilding tree 56 of 100\nbuilding tree 57 of 100\nbuilding tree 58 of 100\nbuilding tree 59 of 100\nbuilding tree 60 of 100\nbuilding tree 61 of 100\nbuilding tree 62 of 100\nbuilding tree 63 of 100\nbuilding tree 64 of 100\nbuilding tree 65 of 100\nbuilding tree 66 of 100\nbuilding tree 67 of 100\nbuilding tree 68 of 100\nbuilding tree 69 of 100\nbuilding tree 70 of 100\nbuilding tree 71 of 100\nbuilding tree 72 of 100\nbuilding tree 73 of 100\nbuilding tree 74 of 100\nbuilding tree 75 of 100\nbuilding tree 76 of 100\nbuilding tree 77 of 100\nbuilding tree 78 of 100\nbuilding tree 79 of 100\nbuilding tree 80 of 100\nbuilding tree 81 of 100\nbuilding tree 82 of 100\nbuilding tree 83 of 100\nbuilding tree 84 of 100\nbuilding tree 85 of 100\nbuilding tree 86 of 100\nbuilding tree 87 of 100\nbuilding tree 88 of 100\nbuilding tree 89 of 100\nbuilding tree 90 of 100\nbuilding tree 91 of 100\nbuilding tree 92 of 100\nbuilding tree 93 of 100\nbuilding tree 94 of 100\nbuilding tree 95 of 100\nbuilding tree 96 of 100\nbuilding tree 97 of 100\nbuilding tree 98 of 100\nbuilding tree 99 of 100\nbuilding tree 100 of 100\n\n\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 25.3min finished\n\n\nRandomForestClassifier(max_leaf_nodes=32, n_jobs=-1, verbose=2)\n\n\n\ntest_pred = rnd_clf.predict(X_test)\ntest_pred = np.rint(test_pred)\ntest_acc = np.sum(test_pred == y_test)/(y_test.shape[0]*5)\nprint(\"test accuracy: \" + str(test_acc))\n\n[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    0.1s\n\n\ntest accuracy: 0.5242761692650334\n\n\n[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.3s finished\n\n\n\n#hide_input\nfrom sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y_test[:,3], test_pred[:,3])\nresp_1 = confusion_matrix(y_test[:,0], test_pred[:,0])\nresp_2 = confusion_matrix(y_test[:,1], test_pred[:,1])\nresp_3 = confusion_matrix(y_test[:,2], test_pred[:,2])\nresp_4 = confusion_matrix(y_test[:,4], test_pred[:,4])\nall_resp = np.add(np.add(np.add(conf_mat, resp_1),np.add(resp_2, resp_3)),resp_4)\nsns.heatmap(all_resp, cmap=\"RdYlGn\", annot=True).set_title(\"All resp summed confusion matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nprint(\"resp\")\nprint(conf_mat)\nprint(\"resp_1\")\nprint(resp_1)\nprint(\"resp_2\")\nprint(resp_2)\nprint(\"resp_3\")\nprint(resp_3)\nprint(\"resp_4\")\nprint(resp_4)\n\nresp\n[[3474 4349]\n [3205 4687]]\nresp_1\n[[4261 3487]\n [3768 4199]]\nresp_2\n[[3740 4001]\n [3367 4607]]\nresp_3\n[[2823 4959]\n [2643 5290]]\nresp_4\n[[3003 4858]\n [2743 5111]]\n\n\n\n\n\n\n\n\n\n\n\nResult 1 justification\nSo we got about 52.4% accuracy with random forest.  From the confusion matrix, we can tell that the model is having harder time predicting 0‚Äôs correctly. It is actually doing a good job of classifying 1‚Äôs though! So with this model, we can expect to get lots of good trades but also fail to not go for bad trades.\n\nResult 1 implementation\nThis was our first pass solution. Although we were able to get a positive score of 52.4%, when submitted to Jane Street for Evaluation, it returned a score of 0. Meaning we have lost more profit than we gained. (The competition didn‚Äôt return negative scores and only calculated positive gains). This suggests that although we were able to get more ‚Äòcorrect‚Äô trades, the scale of the trades we failed to predict correctly have out-weighted our correct predictions.\n\n\n\nMLP evaluation and validation\nClassic multiple layer perceptron with AUC(Area Under Curve) metrics. After looking at many notebooks on Kaggle, MLP seem to perform the best with short run time. Let us build one ourselves.\n\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\n\n#hide_output\nbatch_size = 4096\nhidden_units = [150, 150, 150]\ndropout_rates = [0.20, 0.20, 0.20, 0.20]\nlabel_smoothing = 1e-2\nlearning_rate = 3e-3\n\n#with tpu_strategy.scope():\nclf = create_mlp(\n    X_train.shape[1], 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\n\nclf.fit(X_train, y_train, epochs=100, batch_size=batch_size)\n\n\nmodels = []\n\nmodels.append(clf)\n\nEpoch 1/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6933 - AUC: 0.5320\nEpoch 2/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6897 - AUC: 0.5434\nEpoch 3/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6891 - AUC: 0.5466\nEpoch 4/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6887 - AUC: 0.5483\nEpoch 5/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6885 - AUC: 0.5498\nEpoch 6/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6882 - AUC: 0.5512\nEpoch 7/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6880 - AUC: 0.5519\nEpoch 8/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6878 - AUC: 0.5532\nEpoch 9/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6877 - AUC: 0.5536\nEpoch 10/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6875 - AUC: 0.5541\nEpoch 11/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6872 - AUC: 0.5552\nEpoch 12/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6871 - AUC: 0.5557\nEpoch 13/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6870 - AUC: 0.5562\nEpoch 14/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6868 - AUC: 0.5568\nEpoch 15/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6866 - AUC: 0.5578\nEpoch 16/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6866 - AUC: 0.5577\nEpoch 17/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6864 - AUC: 0.5583\nEpoch 18/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6863 - AUC: 0.5586\nEpoch 19/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6862 - AUC: 0.5591\nEpoch 20/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6861 - AUC: 0.5595\nEpoch 21/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6860 - AUC: 0.5598\nEpoch 22/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6859 - AUC: 0.5602\nEpoch 23/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6858 - AUC: 0.5603\nEpoch 24/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6857 - AUC: 0.5611\nEpoch 25/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6856 - AUC: 0.5608\nEpoch 26/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6855 - AUC: 0.5613\nEpoch 27/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6854 - AUC: 0.5616\nEpoch 28/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6854 - AUC: 0.5616\nEpoch 29/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6853 - AUC: 0.5618\nEpoch 30/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6851 - AUC: 0.5625\nEpoch 31/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6851 - AUC: 0.5625\nEpoch 32/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6850 - AUC: 0.5628\nEpoch 33/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6850 - AUC: 0.5629\nEpoch 34/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6849 - AUC: 0.5633\nEpoch 35/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6849 - AUC: 0.5631\nEpoch 36/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6848 - AUC: 0.5634\nEpoch 37/100\n380/380 [==============================] - 4s 11ms/step - loss: 0.6847 - AUC: 0.5639\nEpoch 38/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6847 - AUC: 0.5639\nEpoch 39/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6846 - AUC: 0.5641\nEpoch 40/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6846 - AUC: 0.5641\nEpoch 41/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6846 - AUC: 0.5641\nEpoch 42/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6844 - AUC: 0.5646\nEpoch 43/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6845 - AUC: 0.5645\nEpoch 44/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6844 - AUC: 0.5645\nEpoch 45/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6843 - AUC: 0.5652\nEpoch 46/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6843 - AUC: 0.5652\nEpoch 47/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6842 - AUC: 0.5651\nEpoch 48/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6842 - AUC: 0.5654\nEpoch 49/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6842 - AUC: 0.5654\nEpoch 50/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6841 - AUC: 0.5656\nEpoch 51/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6841 - AUC: 0.5658\nEpoch 52/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6840 - AUC: 0.5661\nEpoch 53/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6840 - AUC: 0.5656\nEpoch 54/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6840 - AUC: 0.5659\nEpoch 55/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6839 - AUC: 0.5659\nEpoch 56/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6838 - AUC: 0.5661\nEpoch 57/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6839 - AUC: 0.5661\nEpoch 58/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6838 - AUC: 0.5664\nEpoch 59/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6838 - AUC: 0.5664\nEpoch 60/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6838 - AUC: 0.5664\nEpoch 61/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6836 - AUC: 0.5672\nEpoch 62/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6837 - AUC: 0.5667\nEpoch 63/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6837 - AUC: 0.5667\nEpoch 64/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6836 - AUC: 0.5671\nEpoch 65/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6836 - AUC: 0.5670\nEpoch 66/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6837 - AUC: 0.5667\nEpoch 67/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6836 - AUC: 0.5669\nEpoch 68/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6835 - AUC: 0.5672\nEpoch 69/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6835 - AUC: 0.5675\nEpoch 70/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6835 - AUC: 0.5672\nEpoch 71/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6834 - AUC: 0.5673\nEpoch 72/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6833 - AUC: 0.5680\nEpoch 73/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6834 - AUC: 0.5676\nEpoch 74/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6834 - AUC: 0.5672\nEpoch 75/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6833 - AUC: 0.5675\nEpoch 76/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6833 - AUC: 0.5678\nEpoch 77/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6833 - AUC: 0.5678\nEpoch 78/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6832 - AUC: 0.5680\nEpoch 79/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6832 - AUC: 0.5683\nEpoch 80/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6832 - AUC: 0.5680\nEpoch 81/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6833 - AUC: 0.5678\nEpoch 82/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6832 - AUC: 0.5681\nEpoch 83/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6831 - AUC: 0.5682\nEpoch 84/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6831 - AUC: 0.5682\nEpoch 85/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6832 - AUC: 0.5680\nEpoch 86/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6832 - AUC: 0.5683\nEpoch 87/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6830 - AUC: 0.5686\nEpoch 88/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6830 - AUC: 0.5687\nEpoch 89/100\n380/380 [==============================] - 4s 10ms/step - loss: 0.6830 - AUC: 0.5685\nEpoch 90/100\n380/380 [==============================] - 4s 9ms/step - loss: 0.6830 - AUC: 0.5686\nEpoch 91/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6830 - AUC: 0.5688\nEpoch 92/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6829 - AUC: 0.5687\nEpoch 93/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6830 - AUC: 0.5688\nEpoch 94/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6829 - AUC: 0.5688\nEpoch 95/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6830 - AUC: 0.5690\nEpoch 96/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6829 - AUC: 0.5690\nEpoch 97/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6829 - AUC: 0.5690\nEpoch 98/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6828 - AUC: 0.5690\nEpoch 99/100\n380/380 [==============================] - 3s 9ms/step - loss: 0.6828 - AUC: 0.5690\nEpoch 100/100\n380/380 [==============================] - 3s 8ms/step - loss: 0.6828 - AUC: 0.5691\n\n\n\ntest_pred = clf.predict(X_test)\ntest_pred = np.rint(test_pred)\ntest_acc = np.sum(test_pred == y_test)/(y_test.shape[0]*5)\nprint(\"test accuracy: \" + str(test_acc))\n\ntest accuracy: 0.5501368119630926\n\n\n\n#hide_input\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\n\n# calculate scores\nns_auc = roc_auc_score(test_pred[:,3], y_test[:,3])\nresp_1 = roc_auc_score(test_pred[:,0], y_test[:,0])\nresp_2 = roc_auc_score(test_pred[:,1], y_test[:,1])\nresp_3 = roc_auc_score(test_pred[:,2], y_test[:,2])\nresp_4 = roc_auc_score(test_pred[:,4], y_test[:,4])\n# summarize scores\nprint('Resp: ROC AUC=%.3f' % (ns_auc))\nprint('Resp_1: ROC AUC=%.3f' % (resp_1))\nprint('Resp_2: ROC AUC=%.3f' % (resp_2))\nprint('Resp_3: ROC AUC=%.3f' % (resp_3))\nprint('Resp_4: ROC AUC=%.3f' % (resp_4))\n# false positive rate, true positive rate\nns_fpr, ns_tpr, _ = roc_curve(test_pred[:,3], y_test[:,3])\nfpr_1, tpr_1, _ = roc_curve(test_pred[:,0], y_test[:,0])\nfpr_2, tpr_2, _ = roc_curve(test_pred[:,1], y_test[:,1])\nfpr_3, tpr_3, _ = roc_curve(test_pred[:,2], y_test[:,2])\nfpr_4, tpr_4, _ = roc_curve(test_pred[:,4], y_test[:,4])\n\n# plot the roc curve for the model\npyplot.plot(ns_fpr, ns_tpr, label='resp')\npyplot.plot(fpr_1, tpr_1, label='resp_1')\npyplot.plot(fpr_2, tpr_2, label='resp_2')\npyplot.plot(fpr_3, tpr_3, label='resp_3')\npyplot.plot(fpr_4, tpr_4, label='resp_4')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.figure(figsize=(30,20))\npyplot.show()\n\nResp: ROC AUC=0.544\nResp_1: ROC AUC=0.559\nResp_2: ROC AUC=0.551\nResp_3: ROC AUC=0.549\nResp_4: ROC AUC=0.547\n\n\n\n\n\n\n\n\n\n&lt;Figure size 2160x1440 with 0 Axes&gt;\n\n\n\n\nResult 2 justification\nThis is actually good! Although one could say that the machine is doing slightly better than me if I was to go to Jane Street and randomly decide to ‚Äòaction‚Äô on trades. \nIt is important to note that even though we are getting only around ~55% accuracy only, this is actually considered good for trading markets. To explain this, since Jane Market has billions of money, as long as they have a positive return rate, it doesn‚Äôt matter how much they lose because in the end they will gain more. It is like going to a casino knowing you have more chance of winning than losing. The more time you spend here, the more you will gain out of it!\n\n\nHyper-parameter tuning / Refinement\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nbatch_size = 5000\nhidden_units = [(150, 150, 150), (100,100,100), (200,200,200)]\ndropout_rates = [(0.25, 0.25, 0.25, 0.25), (0.3,0.3,0.3,0.3)]\nepochs = 100\nnum_columns = len(features)\nnum_labels = 5\n#num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n\nmlp_CV = KerasClassifier(build_fn=create_mlp, epochs=epochs, batch_size=batch_size, verbose=1)\n\nparam_distributions = {'hidden_units':hidden_units, 'learning_rate':[1e-3, 1e-4], \n                  'label_smoothing':[1e-2, 1e-1], 'dropout_rates':dropout_rates,\n                      'num_columns': [len(features)], 'num_labels': [5]}\n\nrandom_cv = RandomizedSearchCV(estimator=mlp_CV, \n                               param_distributions=param_distributions, n_iter=5,\n                               n_jobs=-1, cv=3, random_state=42)\n\nrandom_cv.fit(X_train, y_train, callbacks=[EarlyStopping(patience=10)])#, epochs=200, batch_size=5000)\n\n\nmodels = []\n\nmodels.append(random_cv)\n\nRandomSearch and GridSearch easily runs out of memory..\nSo from trial and error, I‚Äôve learned that with learning rate at 1e-3, model overfits quickly around at 10 with batch_size around 5000. However, the model wasn‚Äôt able to learn much with less than 100 epochs. One solution is to add more layers and perceptrons which is what I did and the result 2 is the result of manual hyper param tuning. Before the model was definetly at around 200 epochs with same learning rate with 5000 batches giving me an accuracy of only 51%. After manual hyperparameter, (running few different param combination by myself) I was able to increase about 3.5% accuracy!\n\n\nConclusion\nFor my final review and conclusion, check out my blog post\nOther things to try/explore: 1. Weighted training. We know that sometimes we will encounter ‚Äòmonster‚Äô deals. It is crucial for the Kaggle competition to get these ones correct since these will probably outweight most other trades. So we could make model that focuses more on these heavy trades. (high weight X resp data) 2. Split data and train multiple models. Idea is that we could split the data into two by feature_0 and maybe one model that optimizes the ‚Äò1‚Äôs data and another model that optimizes the‚Äô-1‚Äôs data. 3. Make much more features and explore more data (requires time and big data machines) 4. One interesting thing I learned is that apparently, in financial, it is sometimes good to heavily overfit the model. Something to do with volatile. I‚Äôve experimented with this and indeed my utility score for the competition went really high when super overfitted with epoches over 200.\n\n\nSubmission\n\nth = 0.5\n\nf = np.median\nmodels = models[-3:]\n\n\ndef feature_engineering(df):\n    tag_0_group = [9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, 123]\n    for col in tag_0_group:\n        df['log_'+str(col)] = (df['feature_'+str(col)]-df['feature_'+str(col)].min()+1).transform(np.log)\n    return df\n\n\n#hide_output\nimport janestreet\n#env = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() &gt; 0:\n        x_tt = test_df.loc[:, features]\n        x_tt = feature_engineering(x_tt).values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred &gt;= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)\n\n0it [00:00, ?it/s]\n\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-32-00d44d4b25d6&gt; in &lt;module&gt;\n      1 import janestreet\n      2 #env = janestreet.make_env()\n----&gt; 3 for (test_df, pred_df) in tqdm(env.iter_test()):\n      4     if test_df['weight'].item() &gt; 0:\n      5         x_tt = test_df.loc[:, features].values\n\n/opt/conda/lib/python3.7/site-packages/tqdm/std.py in __iter__(self)\n   1125 \n   1126         try:\n-&gt; 1127             for obj in iterable:\n   1128                 yield obj\n   1129                 # Update and possibly print the progressbar.\n\n/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so in iter_test()\n\nException: You can only iterate over `iter_test()` once.\n\n\n\n\n\nReference\nImputing-missing-values  OWN Jane Street with Keras NN"
  },
  {
    "objectID": "posts/2021-01-15-Recommendations.html",
    "href": "posts/2021-01-15-Recommendations.html",
    "title": "Recommendations with IBM üò∫",
    "section": "",
    "text": "Udacity project with IBM Watson Studio platform data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport project_tests as t\nimport pickle\n\n%matplotlib inline\n\ndf = pd.read_csv('data/user-item-interactions.csv')\ndf_content = pd.read_csv('data/articles_community.csv')\ndel df['Unnamed: 0']\ndel df_content['Unnamed: 0']\n\n# Show df to get an idea of the data\ndf.head()\n\n\n\n\n\n\n\n\narticle_id\ntitle\nemail\n\n\n\n\n0\n1430.0\nusing pixiedust for fast, flexible, and easier...\nef5f11f77ba020cd36e1105a00ab868bbdbf7fe7\n\n\n1\n1314.0\nhealthcare python streaming application demo\n083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b\n\n\n2\n1429.0\nuse deep learning for image classification\nb96a4f2e92d8572034b1e9b28f9ac673765cd074\n\n\n3\n1338.0\nml optimization using cognitive assistant\n06485706b34a5c9bf2a0ecdac41daf7e7654ceb7\n\n\n4\n1276.0\ndeploy your python model as a restful api\nf01220c46fc92c6e6b161b1849de11faacd7ccb2\n# Show df_content to get an idea of the data\ndf_content.head()\n\n\n\n\n\n\n\n\ndoc_body\ndoc_description\ndoc_full_name\ndoc_status\narticle_id\n\n\n\n\n0\nSkip navigation Sign in SearchLoading...\\r\\n\\r...\nDetect bad readings in real time using Python ...\nDetect Malfunctioning IoT Sensors with Streami...\nLive\n0\n\n\n1\nNo Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...\nSee the forest, see the trees. Here lies the c...\nCommunicating data science: A guide to present...\nLive\n1\n\n\n2\n‚ò∞ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...\nHere‚Äôs this week‚Äôs news in Data Science and Bi...\nThis Week in Data Science (April 18, 2017)\nLive\n2\n\n\n3\nDATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...\nLearn how distributed DBs solve the problem of...\nDataLayer Conference: Boost the performance of...\nLive\n3\n\n\n4\nSkip navigation Sign in SearchLoading...\\r\\n\\r...\nThis video demonstrates the power of IBM DataS...\nAnalyze NY Restaurant data using Spark in DSX\nLive\n4"
  },
  {
    "objectID": "posts/2021-01-15-Recommendations.html#conclusion",
    "href": "posts/2021-01-15-Recommendations.html#conclusion",
    "title": "Recommendations with IBM üò∫",
    "section": "Conclusion",
    "text": "Conclusion\nWe can see that as the number of latent feature increases, the accuracy decreases. Which is a sign of overfitting since this is a plot of the test set. This can be explained by the small number of users who have both testing and training datasets. We conclude that it is robust enough to decide if our model is ready for deployment. To fix this problem, we can collect more data or use regularizations. In addition we can perform an online A/B testing to measure whether rank based recommendation system or matrix recommendation performs better. Note that our accuracy metrics may not be the best measure of our performance since it is so skewed that by just hard coding, we can correctly guess all except 20. Better metric to use may be precision/recall.\nMake html notebook\n\nfrom subprocess import call\ncall(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])\n\n0"
  },
  {
    "objectID": "posts/2020-12-30-lolpredictb.html",
    "href": "posts/2020-12-30-lolpredictb.html",
    "title": "LoL Prediction S11 üó°Ô∏è",
    "section": "",
    "text": "League of Legends s11 Ranked Prediction\n\n\ntoc: true\nbadges: true\ncomments: true\nauthor: Jaekang Lee\nimage: images/janna.jpg\ncategories: [python, jupyter, CRISP-DM, League of Legends, Linear Regression, Random Forest, Udacity]\n\n\n\nimage src: Riot Games\n\n\nIntroduction\nRiot Games brings massive changes to their game ‚ÄòLeague of Legend‚Äô every year. This year, they changed their item system, drastically changing their game ecosystem. It has been few months since the big update and now players have fully adapted to the changes. Let‚Äôs take a look at what happened to the ecosystem and what is the best team composition now.\n\nFind out what are the most popular champions now.\nFind out which team composition is the best.\nCompare Season 10 and pre-Season 11. How did the item changes impact the game?\n\n\n\nThe dataset\nThe data we are going to use is a csv file obtained from scraping op.gg which is a website with League of Legend statistics. If you are interested you can visit here. The dataset consists of 2901 ranked matches from Korea(WWW), North America(NA), Eastern Europe(EUNE), and Western Europe(EUW) servers. It has which team won the match, the total time of the match, blue team composition and red team composition. Note that only the high elo games were added this includes Challenger, Grand Master, Master and sometimes even High Diamonds. Note that there are 153 total unique champions with ‚ÄòRell‚Äô as the latest addition. Duplicate games have been removed.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport seaborn as sns\nfrom sklearn.utils import shuffle\n%matplotlib inline\n\n\ndf = pd.read_csv ('data/s11.csv')\ndf = shuffle(df)\ndf.head()\n\n\n\n\n\n\n\n\nresult\nserver\nteam_1__001\nteam_1__002\nteam_1__003\nteam_1__004\nteam_1__005\nteam_2__001\nteam_2__002\nteam_2__003\nteam_2__004\nteam_2__005\ntimestamp\ngame_length\n\n\n\n\n1265\nDefeat\nwww\nCamille\nHecarim\nNeeko\nAphelios\nSett\nRumble\nKayn\nTwisted Fate\nMiss Fortune\nLeona\n2020-12-07 18:48:25\n34m 23s\n\n\n1958\nDefeat\neune\nKennen\nRengar\nKassadin\nMiss Fortune\nBard\nKayle\nGraves\nFiora\nCaitlyn\nThresh\n2020-12-18 18:26:55\n16m 12s\n\n\n1877\nVictory\neuw\nMordekaiser\nOlaf\nZoe\nJhin\nAlistar\nShen\nHecarim\nLeBlanc\nAphelios\nGalio\n2020-12-30 08:58:13\n34m 27s\n\n\n778\nVictory\nwww\nAatrox\nElise\nLucian\nMiss Fortune\nPantheon\nCamille\nGraves\nZoe\nJhin\nLeona\n2020-12-29 21:49:55\n18m 56s\n\n\n2591\nDefeat\nna\nPoppy\nKayn\nAkali\nSenna\nBraum\nVolibear\nOlaf\nYone\nTwisted Fate\nJanna\n2020-11-10 07:38:39\n31m 15s\n\n\n\n\n\n\n\n\n\nData Cleaning\n\nChange game_length to continuous variable\nClean null values and uninformative columns\nChange categorical variables to dummy variables\n\n\n# Convert game_length(str) to float(seconds)\nimport re\ndate_str = df.game_length\n\nfor i in range(len(date_str)):\n    if type(date_str[i]) == str:\n        p = re.compile('\\d*')\n        min = float(p.findall(date_str[i][:2])[0])\n        temp = p.findall(date_str[i][-3:])\n        for j in temp:\n            if j != '':\n                sec = float(j)\n                break\n        date_str[i] = (60*min+sec)\n    else: \n        date_str[i] = date_str[i]\n#     print(date_str[i])\n# print(len(date_str))\n\n# remove timestamp since it does not affect the game\ndf = df.drop(['timestamp'], axis=1)\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nresult\nserver\nteam_1__001\nteam_1__002\nteam_1__003\nteam_1__004\nteam_1__005\nteam_2__001\nteam_2__002\nteam_2__003\nteam_2__004\nteam_2__005\ngame_length\n\n\n\n\ncount\n2901\n2901\n2901\n2901\n2901\n2901\n2901\n2901\n2901\n2901\n2901\n2901\n2901.0\n\n\nunique\n2\n4\n96\n62\n102\n70\n56\n95\n58\n102\n72\n63\n235.0\n\n\ntop\nDefeat\nwww\nCamille\nGraves\nAkali\nJhin\nLeona\nCamille\nGraves\nYone\nKai'Sa\nLeona\n1818.0\n\n\nfreq\n2271\n1592\n305\n581\n235\n590\n355\n266\n504\n226\n568\n381\n100.0\n\n\n\n\n\n\n\n\n\nMost popular champions\n\nCamille(Top): 19.68% pick rate\nGraves(Jg): 37.4% pick rate\nAkali/Yone(Mid): 15.89% pick rate combined\nJhin/Kai‚Äôsa(Adc): 39.92% pick rate combined\nLeona(Supp): 25.37% pick rate \n\nNotes: - The result is very skewed because there are 2271 Red Team win compared to only 630 Blue Team wins - There are in total 2901 games and more than half of it is from Korean server\n\n# see if there are any null values\nno_nulls = set(df.columns[df.isnull().sum()==0])\nprint(no_nulls)\n\n{'result', 'server', 'team_1__004', 'team_2__003', 'team_1__001', 'team_2__005', 'team_1__003', 'team_2__001', 'game_length', 'team_1__002', 'team_1__005', 'team_2__004', 'team_2__002'}\n\n\nSo there are no null values which is good!\n\n# Categorical Columns\ncat_cols = ['result', 'server', 'team_1__004', 'team_2__003', 'team_1__001', 'team_2__005', 'team_1__003', 'team_2__001','team_1__002', 'team_1__005', 'team_2__004', 'team_2__002']\n\ndef create_dummy_df(df, cat_cols):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    cat_cols - list of strings that are associated with names of the categorical columns\n    \n    OUTPUT:\n    df - new dataframe with following characteristics:\n        1. contains all columns that were not specified as categorical\n        2. removes all the original columns in cat_cols\n        3. dummy columns for each of the categorical columns in cat_cols\n        4. Use a prefix of the column name with an underscore (_) for separating\n    '''\n    for col in cat_cols:\n        try:\n            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True)], axis=1)\n        except:\n            continue\n    return df\n\n\ndf = create_dummy_df(df, cat_cols)\n\n\n# Might as well normalize game_length as well since Linear Regression is sensitive to it\nmax_time = max(df['game_length'])\ndf=pd.concat([df.drop('game_length', axis=1), (df['game_length']/max_time)], axis=1)\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\nresult_Victory\nserver_euw\nserver_na\nserver_www\nteam_1__004_Akali\nteam_1__004_Anivia\nteam_1__004_Annie\nteam_1__004_Aphelios\nteam_1__004_Ashe\nteam_1__004_Aurelion Sol\n...\nteam_2__002_Udyr\nteam_2__002_Urgot\nteam_2__002_Vi\nteam_2__002_Volibear\nteam_2__002_Warwick\nteam_2__002_Wukong\nteam_2__002_Xin Zhao\nteam_2__002_Zac\nteam_2__002_Zed\ngame_length\n\n\n\n\n1265\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.700272\n\n\n1958\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.329939\n\n\n1877\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.701629\n\n\n778\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.385608\n\n\n2591\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.636456\n\n\n655\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.517651\n\n\n1089\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.700272\n\n\n1221\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.617108\n\n\n1480\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.419212\n\n\n1791\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.556687\n\n\n\n\n10 rows √ó 771 columns\n\n\n\nThe data is ready for modelling.\n\n\nLinear Regression\n\n# Train test\ny = df['result_Victory']\nX = df.drop(['result_Victory'], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n\nlm_model = LinearRegression(normalize=True)\n\n\nlm_model.fit(X_train, y_train)\n\nLinearRegression(normalize=True)\n\n\n\ntest_pred = lm_model.predict(X_test)\ntrain_pred = lm_model.predict(X_train)\nr2_test = r2_score(y_test, test_pred)\nr2_train = r2_score(y_train, train_pred)\nprint(\"test r2: \"+str(r2_test))\nprint(\"train r2: \"+str(r2_train))\n\ntest r2: -2.7134011717466985e+28\ntrain r2: 0.25972262531839096\n\n\nClearly, linear regression is a poor model for this problem haha. Makes sense since we only have discrete fields except game_length.\n\ndef coef_weights(coefficients, X_train):\n    '''\n    INPUT:\n    coefficients - the coefficients of the linear model \n    X_train - the training data, so the column names can be used\n    OUTPUT:\n    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n    \n    Provides a dataframe that can be used to understand the most influential coefficients\n    in a linear model by providing the coefficient estimates along with the name of the \n    variable attached to the coefficient.\n    '''\n    coefs_df = pd.DataFrame()\n    coefs_df['est_int'] = X_train.columns\n    coefs_df['coefs'] = lm_model.coef_\n    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n    return coefs_df\n\n\ncoef_df = coef_weights(lm_model.coef_, X_train)\ncoef_df.head(20)\n\n\n\n\n\n\n\n\nest_int\ncoefs\nabs_coefs\n\n\n\n\n435\nteam_2__001_Aphelios\n7.451403e+14\n7.451403e+14\n\n\n689\nteam_2__004_Sion\n-7.451403e+14\n7.451403e+14\n\n\n298\nteam_2__005_Ornn\n5.132176e+14\n5.132176e+14\n\n\n248\nteam_1__001_Sona\n-5.052884e+14\n5.052884e+14\n\n\n31\nteam_1__004_Kindred\n-4.224175e+14\n4.224175e+14\n\n\n43\nteam_1__004_Pantheon\n4.184556e+14\n4.184556e+14\n\n\n282\nteam_2__005_Jayce\n4.156125e+14\n4.156125e+14\n\n\n635\nteam_1__005_Yasuo\n3.822057e+14\n3.822057e+14\n\n\n595\nteam_1__005_Ekko\n3.822057e+14\n3.822057e+14\n\n\n412\nteam_1__003_Thresh\n-3.822057e+14\n3.822057e+14\n\n\n79\nteam_2__003_Bard\n3.781834e+14\n3.781834e+14\n\n\n328\nteam_2__005_Zoe\n-3.772819e+14\n3.772819e+14\n\n\n19\nteam_1__004_Gragas\n3.742793e+14\n3.742793e+14\n\n\n548\nteam_1__002_Leona\n-3.660990e+14\n3.660990e+14\n\n\n611\nteam_1__005_Nidalee\n3.660990e+14\n3.660990e+14\n\n\n526\nteam_1__002_Camille\n-3.586479e+14\n3.586479e+14\n\n\n382\nteam_1__003_Miss Fortune\n3.339794e+14\n3.339794e+14\n\n\n5\nteam_1__004_Annie\n3.293351e+14\n3.293351e+14\n\n\n576\nteam_1__002_Twitch\n3.012684e+14\n3.012684e+14\n\n\n292\nteam_2__005_Miss Fortune\n2.971517e+14\n2.971517e+14\n\n\n\n\n\n\n\nRecall that 1 = Blue win and 0 = Red win. So positive coefs. here means helpful for the Blue team and negative coefs. means helpful for the Red team. Most of the fields in the top 20 table above, are not something we see often. For example 435-aphelios(top), 689-sion(adc), 248-sona(top) are considered troll. Here are some other findings. - Looks like every lane is somewhat equally important as their appearance in the table above are similiar - Most of these are troll picks negatively affecting its own team‚Äôs winrate - Picks that are actually helping team‚Äôs winrate: Sion(ADC), Pantheon(ADC), Yasuo(Sup)??, Ekko(Sup)?? - This table raises more questions than answers!\n\n\nRandom Forests\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\n\nRandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1)\n\n\n\ny_test_pred = rnd_clf.predict(X_test)\ntest_acc = np.sum(y_test_pred == y_test)/len(y_test)\nprint(\"test accuracy: \"+str(test_acc))\n\ntest accuracy: 0.8003442340791739\n\n\nWow we went from 0% to 80% accuracy with random forest!\n\nimport shap\n\nexplainer = shap.TreeExplainer(rnd_clf)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values[1], X_test)\n\n\n\n\n\n\n\n\nInterestingly, West Europe tend to win more as Blue team as games are longer. In contrast, Korea tend to win more as Red Team as games gets longer. So there seem to be a trend difference between regions. Furthermore, in general, the shorter the game, blue team wins more for some reason I cannot figure out.\n\n\nBest/Worst Composition\nBest - (Top)Camille,Yone (Jg)Hecarim,Olaf,Twitch (Mid)Akali (Adc)Miss Fortune,Jhin (Sup)Alistar,Janna,Leona \nWorst - (Top)Pantheon,Irelia (Jg)Wukong (Mid)Sylas,Yone\nIf we compare this with the official na.op.gg champion rankings, all the best champions listed here are also listed on their website as either tier one or two as well. (Except Twitch and Pantheon).  Note that this is just for comparison. Op.gg has million times more data with more regions. Also how they rank these champions are not revealed.\n\n\nComparing with S10\n\nBest team composition\n\n\n#hide_input\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n#hide_input\nmonty = PILImage.create(\"images/good.jpg\")\ndisplay(monty)\n\n\n\n\n\n\n\n\n\nWorst team composition\n\n\n#hide_input\nmonty = PILImage.create(\"images/bad.jpg\")\ndisplay(monty)\n\n\n\n\n\n\n\n\nComparisons - The new update caused each roles to impact more evenly to the game‚Äôs result - Bottom lane has generally good picks with no worst picks in season 11. - The new update caused more ‚Äòhigh risk high reward‚Äô champions to win more and ‚Äògenerally good‚Äô champions to fall"
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html",
    "title": "Deep Art Gallery üß†",
    "section": "",
    "text": "Neural Style Transfer, ConvNet(VGG19), Transfer Learning, tf-gpu"
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html#styles-used",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html#styles-used",
    "title": "Deep Art Gallery üß†",
    "section": "Styles used",
    "text": "Styles used\n I think you can guess which styles were used on which photos. Except Newbie‚Äôs style is by Wassily Kandinsky ‚Äî Composition VII"
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html#how-it-works",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html#how-it-works",
    "title": "Deep Art Gallery üß†",
    "section": "How it works",
    "text": "How it works\nNeural Style Transfer works by choosing a content image and a style image and then ‚Äòdrawing‚Äô the content image using style of the style image.\nIn implementation, all we are doing is calculating some derivatives to make a number small as possible.\nThis is the cost function we are trying to minimize. As \\(J(GeneratedImage)\\) gets smaller, we get the art we want. Think of cost function as distance from our art being beautiful. G is initialized as a random noise image. We will use Adam optimization to compute the gradient. Think of gradient as small step towards prettiness.\n\nSo every iteration, G will be subtracted with gradient of \\(J(GeneratedImage)\\) slowly becoming beautiful.\n\n\nContent Cost Function \\(J_{content}(C,G)\\)\n\\[J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} \\]\n\nHere, \\(a\\) stands for activation of the lth layer in our convNet.\n\\(n_H\\), \\(n_W\\), \\(n_C\\) is the dimension of the layer. (Height, width, depth).\nThe constants in front are just for normalization.\n\n\n\nStyle Cost Function \\(J_{style}(S,G)\\)\n\\[J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} \\]\n\nThe constants in front are just for normalization\nThe gram is a function that just calculates the correlation between horizontal vectors in the given matrix(which is our depths)\nWe will calculate gram of activation layer from both content and generated layer for all combinations of depths(i,j).\nAnd this is just one layer. Then we compute for all layers. This is why it takes so long to generate our image.\nNote that the picture below ‚Äòunrolled‚Äô a 3d volume into 2d matrix. \nAs you can see style cost function is less straightforward. ‚ÄúIf you don‚Äôt understand it, don‚Äôt worry about it‚Äù - Andrew NG."
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html#code",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html#code",
    "title": "Deep Art Gallery üß†",
    "section": "Code",
    "text": "Code\nCred to Tensorflow (see reference) Note that most of the arts generated above were using code from a coursera assignment which is different from codes below showing implementation (same but different transferred model) in tensorflow2. Modified to run on gpu.\n\n# hide\nimport os\nimport sys\nimport scipy.io\nimport scipy.misc\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport PIL.Image\n#from nst_utils import *\nimport numpy as np\nimport tensorflow as tf\nimport pprint\n%matplotlib inline\nfrom IPython import display\nimport imageio\n\n\n# hide\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\n# hide\ndef load_img(path_to_img):\n  max_dim = 512\n  img = tf.io.read_file(path_to_img)\n  img = tf.image.decode_image(img, channels=3)\n  img = tf.image.convert_image_dtype(img, tf.float32)\n\n  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n  long_dim = max(shape)\n  scale = max_dim / long_dim\n\n  new_shape = tf.cast(shape * scale, tf.int32)\n\n  img = tf.image.resize(img, new_shape)\n  img = img[tf.newaxis, :]\n  return img\n\n\ncontent_image = load_img('images/newby.jpg')\nstyle_image = load_img('images/kandinsky.jpg')\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\n\n\n\n\n\n\n\n\ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)"
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html#transfer-learning",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html#transfer-learning",
    "title": "Deep Art Gallery üß†",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nChoice for the model is VGG19 since it is what was used in the original paper by Leon A. Gatys, Alexander S. Ecker, Matthias Bethge.\n\ncontent_layers = ['block5_conv2'] \n\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)\n\n\ndef vgg_layers(layer_names):\n  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n  # Load our model. Load pretrained VGG, trained on imagenet data\n  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n  vgg.trainable = False\n\n  outputs = [vgg.get_layer(name).output for name in layer_names]\n\n  model = tf.keras.Model([vgg.input], outputs)\n  return model\n\n\nstyle_extractor = vgg_layers(style_layers)\nstyle_outputs = style_extractor(style_image*255)\n\n#Look at the statistics of each layer's output\n# for name, output in zip(style_layers, style_outputs):\n#   print(name)\n#   print(\"  shape: \", output.numpy().shape)\n#   print(\"  min: \", output.numpy().min())\n#   print(\"  max: \", output.numpy().max())\n#   print(\"  mean: \", output.numpy().mean())\n#   print()\n\n\nHelper functions\n\ndef gram_matrix(input_tensor):\n  result = tf.linalg.einsum('bijc,bijd-&gt;bcd', input_tensor, input_tensor)\n  input_shape = tf.shape(input_tensor)\n  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n  return result/(num_locations)\n\n\nclass StyleContentModel(tf.keras.models.Model):\n  def __init__(self, style_layers, content_layers):\n    super(StyleContentModel, self).__init__()\n    self.vgg =  vgg_layers(style_layers + content_layers)\n    self.style_layers = style_layers\n    self.content_layers = content_layers\n    self.num_style_layers = len(style_layers)\n    self.vgg.trainable = False\n\n  def call(self, inputs):\n    \"Expects float input in [0,1]\"\n    inputs = inputs*255.0\n    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n    outputs = self.vgg(preprocessed_input)\n    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n                                      outputs[self.num_style_layers:])\n\n    style_outputs = [gram_matrix(style_output)\n                     for style_output in style_outputs]\n\n    content_dict = {content_name:value \n                    for content_name, value \n                    in zip(self.content_layers, content_outputs)}\n\n    style_dict = {style_name:value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n\n    return {'content':content_dict, 'style':style_dict}\n\n\nextractor = StyleContentModel(style_layers, content_layers)\n\nresults = extractor(tf.constant(content_image))\n\n# print('Styles:')\n# for name, output in sorted(results['style'].items()):\n#   print(\"  \", name)\n#   print(\"    shape: \", output.numpy().shape)\n#   print(\"    min: \", output.numpy().min())\n#   print(\"    max: \", output.numpy().max())\n#   print(\"    mean: \", output.numpy().mean())\n#   print()\n\n# print(\"Contents:\")\n# for name, output in sorted(results['content'].items()):\n#   print(\"  \", name)\n#   print(\"    shape: \", output.numpy().shape)\n#   print(\"    min: \", output.numpy().min())\n#   print(\"    max: \", output.numpy().max())\n#   print(\"    mean: \", output.numpy().mean())\n\n\nstyle_targets = extractor(style_image)['style']\ncontent_targets = extractor(content_image)['content']\n\n\nimage = tf.Variable(content_image)\n\n\ndef clip_0_1(image):\n  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\n\nopt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n\n\nstyle_weight=1e-2\ncontent_weight=1e4\n\n\ndef style_content_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n                           for name in style_outputs.keys()])\n    style_loss *= style_weight / num_style_layers\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n                             for name in content_outputs.keys()])\n    content_loss *= content_weight / num_content_layers\n    loss = style_loss + content_loss\n    return loss\n\n\n@tf.function()\ndef train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))"
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html#training",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html#training",
    "title": "Deep Art Gallery üß†",
    "section": "Training",
    "text": "Training\n\ntrain_step(image)\ntrain_step(image)\ntrain_step(image)\ntensor_to_image(image)\n\n\n\n\n\n\n\n\nDo 1000 iteration and save every 200th iteration image\n\nimport time\nwith tf.device(\"/gpu:0\"):\n    start = time.time()\n    epochs = 10\n    steps_per_epoch = 100\n\n    step = 0\n    for n in range(epochs):\n      for m in range(steps_per_epoch):\n        step += 1\n        train_step(image)\n        print(\".\", end='')\n      display.clear_output(wait=True)\n      display.display(tensor_to_image(image))\n      print(\"Train step: {}\".format(step))\n    \n      # save current generated image in the \"/output\" directory\n      imageio.imwrite(\"output/\" + str(2*100) + \".png\", tensor_to_image(image))\n\n    \n    end = time.time()\n    print(\"Total time: {:.1f}\".format(end-start))\n\n\n\n\n\n\n\n\nTrain step: 1000\nTotal time: 634.3\n\n\n\n# save_image(\"output/\" + str(2*100) + \".png\", tensor_to_image(image))\nimageio.imwrite(\"output/\" + str(2*100) + \".png\", tensor_to_image(image))"
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html#total-variation-loss",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html#total-variation-loss",
    "title": "Deep Art Gallery üß†",
    "section": "Total Variation Loss",
    "text": "Total Variation Loss\nI didn‚Äôt learn this part so its like magic to me\n\ndef high_pass_x_y(image):\n  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n\n  return x_var, y_var\n\n\nx_deltas, y_deltas = high_pass_x_y(content_image)\n\nplt.figure(figsize=(14,10))\nplt.subplot(2,2,1)\nimshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas Original\")\n\nplt.subplot(2,2,2)\nimshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas Original\")\n\nx_deltas, y_deltas = high_pass_x_y(image)\n\nplt.subplot(2,2,3)\nimshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas Styled\")\n\nplt.subplot(2,2,4)\nimshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas Styled\")\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(14,10))\n\nsobel = tf.image.sobel_edges(content_image)\nplt.subplot(1,2,1)\nimshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\nplt.subplot(1,2,2)\nimshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")\n\n\n\n\n\n\n\n\n\ndef total_variation_loss(image):\n  x_deltas, y_deltas = high_pass_x_y(image)\n  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n\n\ntf.image.total_variation(image).numpy()\n\narray([114173.52], dtype=float32)\n\n\n\ntotal_variation_weight=30\n\n\n@tf.function()\ndef train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n    loss += total_variation_weight*tf.image.total_variation(image)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))\n\n\nimage = tf.Variable(content_image)\n\n\nimport time\nwith tf.device(\"/gpu:0\"):\n    start = time.time()\n\n    epochs = 10\n    steps_per_epoch = 100\n\n    step = 0\n    for n in range(epochs):\n      for m in range(steps_per_epoch):\n        step += 1\n        train_step(image)\n        print(\".\", end='')\n      display.clear_output(wait=True)\n      display.display(tensor_to_image(image))\n      print(\"Train step: {}\".format(step))\n\n    end = time.time()\n    print(\"Total time: {:.1f}\".format(end-start))\n\n\n\n\n\n\n\n\nTrain step: 1000\nTotal time: 834.3\n\n\n\nfile_name = 'generated_image.png'\nimageio.imwrite(\"output/\" + 'generated_image' + \".png\", tensor_to_image(image))"
  },
  {
    "objectID": "posts/2020-12-02-Deep-Art-Gallery.html#references",
    "href": "posts/2020-12-02-Deep-Art-Gallery.html#references",
    "title": "Deep Art Gallery üß†",
    "section": "References:",
    "text": "References:\nThe Neural Style Transfer algorithm was due to Gatys et al.¬†(2015). The pre-trained network used in this implementation is a VGG network, which is due to Simonyan and Zisserman (2015). The whole code is basically from tensorflow website listed below with little changes(to save images and use gpu)\n\nLeon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). A Neural Algorithm of Artistic Style\nHarish Narayanan, Convolutional neural networks for artistic style transfer.\nDeepLearningAi(Coursera) (2020). Deep Learning Specialization\nTensorFlow (2019). Neural style transfer"
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html",
    "href": "posts/2020-10-28-lolpredict.html",
    "title": "LoL Prediction S10 üèπ",
    "section": "",
    "text": "LOL s10 high elo ranked games prediction."
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#introduction",
    "href": "posts/2020-10-28-lolpredict.html#introduction",
    "title": "LoL Prediction S10 üèπ",
    "section": "Introduction",
    "text": "Introduction\nLet‚Äôs predict who won the match given team composition and how long game played out"
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#get-dataset",
    "href": "posts/2020-10-28-lolpredict.html#get-dataset",
    "title": "LoL Prediction S10 üèπ",
    "section": "Get dataset",
    "text": "Get dataset\nThe dataset is a collection of League of Legends High Elo(Challenger, GM, Master, High Diamonds) Ranked games in Season 10, Korea(WWW), North America(NA), Eastern Europe(EUNE), and Western Europe(EUW) servers. These datas were collected from op.gg by web scrapping with python spyder. The latest game was played on Oct.16th on the dataset. In total there are 4028 unique games. Note that I‚Äôve used one-hot encoding hence [99,54,101,73,57,96,52,102,68,52] this list represents number of all unique champions used in each lanes [BlueTop, BlueJG, BlueMid, BlueAdc, BlueSup, RedTop, RedJg, RedMid, RedAdc, RedSup] respectivley. Note that there are in total 151 unique champions with ‚ÄòSamira‚Äô as the latest addition.\n\nimport pandas as pd\ndf = pd.read_csv(\"games.csv\")\n\nSome Setups\n\n# Python ‚â•3.5 is required\nimport sys\nassert sys.version_info &gt;= (3, 5)\n\n# Scikit-Learn ‚â•0.20 is required\nimport sklearn\nassert sklearn.__version__ &gt;= \"0.20\"\n\ntry:\n    # %tensorflow_version only exists in Colab.\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\n\n# TensorFlow ‚â•2.0 is required\nimport tensorflow as tf\nfrom tensorflow import keras\nassert tf.__version__ &gt;= \"2.0\"\n\n%load_ext tensorboard\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"deep\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\n\ndf.head(5)  # First look at our dataset. Game_length includes some annoying string values instead of time value\n\n\n\n\n\n\n\n\ngame_length\nmmr\nresult\nserver\nteam_1\nteam_2\ntimestamp\n\n\n\n\n0\n25m 38s\nNaN\nVictory\nna\nRiven,Nidalee,Galio,Jhin,Pantheon\nCamille,Olaf,Cassiopeia,Ezreal,Alistar\n2020-10-13 09:31:42\n\n\n1\n25m 38s\nNaN\nDefeat\nna\nTeemo,Nidalee,Lucian,Caitlyn,Senna\nIrelia,Hecarim,Cassiopeia,Jinx,Lulu\n2020-10-13 06:00:17\n\n\n2\n25m 38s\nNaN\nDefeat\nna\nMalphite,Olaf,Taliyah,Ezreal,Alistar\nSylas,Lillia,Lucian,Senna,Pantheon\n2020-10-13 05:06:45\n\n\n3\n25m 38s\nNaN\nDefeat\nna\nNeeko,Shen,Orianna,Kai'Sa,Nautilus\nRiven,Hecarim,Cassiopeia,Samira,Morgana\n2020-10-13 04:28:00\n\n\n4\n25m 38s\nNaN\nDefeat\nna\nFiora,Nunu & Willump,Irelia,Jhin,Karma\nRenekton,Elise,Kled,Jinx,Morgana\n2020-10-13 04:00:51\n\n\n\n\n\n\n\n\ntemp_df = df[['game_length', 'result', 'team_1', 'team_2']] # Select only interests\nblue = temp_df['team_1']\nred = temp_df['team_2']\nn = len(df)\n\nblue_champs = []\nred_champs = []\nfor i in range(0,n):\n    blue_champs += [blue[i].split(',')]\n    red_champs += [red[i].split(',')]\n    \ntop = []\njg = []\nmid = []\nadc = []\nsup = []\nfor i in range(0, n):\n    top += [blue_champs[i][0]]\n    jg += [blue_champs[i][1]]\n    mid += [blue_champs[i][2]]\n    adc += [blue_champs[i][3]]\n    sup += [blue_champs[i][4]]\n    \ntop_2 = []\njg_2 = []\nmid_2 = []\nadc_2 = []\nsup_2 = []\nfor i in range(0, n):\n    top_2 += [red_champs[i][0]]\n    jg_2 += [red_champs[i][1]]\n    mid_2 += [red_champs[i][2]]\n    adc_2 += [red_champs[i][3]]\n    sup_2 += [red_champs[i][4]]\n\n\ndata = temp_df.drop(columns=['team_1','team_2'])\n# blue team\ndata['top1'] = top\ndata['jg1'] = jg\ndata['mid1'] = mid\ndata['adc1'] = adc\ndata['sup1'] = sup\n# red team\ndata['top2'] = top_2\ndata['jg2'] = jg_2\ndata['mid2'] = mid_2\ndata['adc2'] = adc_2\ndata['sup2'] = sup_2\n\n\ndata.head(10)\n\n\n\n\n\n\n\n\ngame_length\nresult\ntop1\njg1\nmid1\nadc1\nsup1\ntop2\njg2\nmid2\nadc2\nsup2\n\n\n\n\n0\n25m 38s\nVictory\nRiven\nNidalee\nGalio\nJhin\nPantheon\nCamille\nOlaf\nCassiopeia\nEzreal\nAlistar\n\n\n1\n25m 38s\nDefeat\nTeemo\nNidalee\nLucian\nCaitlyn\nSenna\nIrelia\nHecarim\nCassiopeia\nJinx\nLulu\n\n\n2\n25m 38s\nDefeat\nMalphite\nOlaf\nTaliyah\nEzreal\nAlistar\nSylas\nLillia\nLucian\nSenna\nPantheon\n\n\n3\n25m 38s\nDefeat\nNeeko\nShen\nOrianna\nKai'Sa\nNautilus\nRiven\nHecarim\nCassiopeia\nSamira\nMorgana\n\n\n4\n25m 38s\nDefeat\nFiora\nNunu & Willump\nIrelia\nJhin\nKarma\nRenekton\nElise\nKled\nJinx\nMorgana\n\n\n5\n25m 38s\nDefeat\nIrelia\nKarthus\nSylas\nSamira\nNautilus\nRiven\nKayn\nAkali\nMiss Fortune\nGalio\n\n\n6\n25m 38s\nDefeat\nGalio\nKindred\nSyndra\nEzreal\nBlitzcrank\nCamille\nFiddlesticks\nTwisted Fate\nJhin\nMorgana\n\n\n7\n25m 38s\nDefeat\nPoppy\nEkko\nSylas\nSamira\nBlitzcrank\nLucian\nLillia\nLulu\nCaitlyn\nAlistar\n\n\n8\n25m 38s\nDefeat\nShen\nLillia\nSamira\nLucian\nSoraka\nTaric\nMaster Yi\nRiven\nEzreal\nLulu\n\n\n9\n25m 38s\nDefeat\nOrnn\nGraves\nSylas\nLucian\nAlistar\nIrelia\nHecarim\nAkali\nSenna\nLeona\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n#y = pd.get_dummies(data.top1, prefix='top1')\nenc = OneHotEncoder()\nonly_champs = data.drop(columns=['game_length', 'result'])\nonly_champs.head(5)\nonly_champs_onehot = enc.fit_transform(only_champs)\n\n\n# print(only_champs_onehot)\nenc.get_params()\n\n{'categories': 'auto',\n 'drop': None,\n 'dtype': numpy.float64,\n 'handle_unknown': 'error',\n 'sparse': True}\n\n\n\n# Convert game_length to float and normalize\nimport re\ndate_str = data.game_length\nm = 2717 #longest games are 45m 17s\n\nfor i in range(len(date_str)):\n    if type(date_str[i]) == str:\n        p = re.compile('\\d*')\n        min = float(p.findall(date_str[i][:2])[0])\n        temp = p.findall(date_str[i][-3:])\n        for j in temp:\n            if j != '':\n                sec = float(j)\n                break\n        date_str[i] = (60*min+sec)/m\n    else: \n        date_str[i] = date_str[i]/m\n#     print(date_str[i])\n# print(len(date_str))\n\n\n# Now we have the X we want\n#except_champs = data.drop(columns=['result','top1','jg1','mid1','adc1','sup1','top2','jg2','mid2','adc2','sup2'])\nsparse_to_df = pd.DataFrame.sparse.from_spmatrix(only_champs_onehot)\nprint(sparse_to_df.shape)\nprint(date_str.shape)\n\nX = date_str.to_frame().join(sparse_to_df).dropna()\nX = np.asarray(X).astype('float32')\n\n(4028, 754)\n(4028,)\n\n\n\ny = data['result']\nfor i in range(len(y)):\n    if y[i] == \"Victory\":\n        y[i] = 1\n    else:\n        y[i] = 0\n\n\ny = np.asarray(y).astype('float32')"
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#datas-are-one-hot-encoded-and-cleaned-up.-lets-train-test-split",
    "href": "posts/2020-10-28-lolpredict.html#datas-are-one-hot-encoded-and-cleaned-up.-lets-train-test-split",
    "title": "LoL Prediction S10 üèπ",
    "section": "Datas are one hot encoded and cleaned up. Let‚Äôs train test split",
    "text": "Datas are one hot encoded and cleaned up. Let‚Äôs train test split\n\nfrom sklearn.model_selection import train_test_split\nimport math\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n#len(X_train) = 3222\nl = math.floor(3222*0.8)\nX_valid, X_train = X_train_full[:l], X_train_full[l:]\ny_valid, y_train = y_train_full[:l], y_train_full[l:]\nprint(y_valid.shape)\nprint(X_valid.shape)\n\n(2577,)\n(2577, 755)"
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#lets-try-neural-network-with-dropouts",
    "href": "posts/2020-10-28-lolpredict.html#lets-try-neural-network-with-dropouts",
    "title": "LoL Prediction S10 üèπ",
    "section": "Let‚Äôs try Neural Network with dropouts",
    "text": "Let‚Äôs try Neural Network with dropouts\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=(755,)),\n    keras.layers.Dense(30, activation=\"relu\", name=\"layer_1\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(16, activation=\"relu\", name=\"layer_2\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(16, activation=\"relu\", name=\"layer_3\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(1, activation=\"sigmoid\", name=\"layer_4\")\n])\n\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 755)               0         \n_________________________________________________________________\nlayer_1 (Dense)              (None, 30)                22680     \n_________________________________________________________________\ndropout (Dropout)            (None, 30)                0         \n_________________________________________________________________\nlayer_2 (Dense)              (None, 16)                496       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\nlayer_3 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 16)                0         \n_________________________________________________________________\nlayer_4 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 23,465\nTrainable params: 23,465\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.fit(X_train, y_train, epochs=50, batch_size=1)\n\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('accuracy', test_acc)\n\n26/26 [==============================] - 0s 806us/step - loss: 3.8032 - accuracy: 0.6613\naccuracy 0.6612903475761414\n\n\nWe got about 0.661 accuracy with just raw neural network with dropouts."
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#lets-try-random-forests",
    "href": "posts/2020-10-28-lolpredict.html#lets-try-random-forests",
    "title": "LoL Prediction S10 üèπ",
    "section": "Let‚Äôs try random forests",
    "text": "Let‚Äôs try random forests\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\n\nRandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1)\n\n\n\ny_val_pred = rnd_clf.predict(X_valid)\n\n\nval_acc = np.sum(y_val_pred == y_valid)/len(y_valid)\nprint(\"validation accuracy: \"+str(val_acc))\n\nvalidation accuracy: 0.7710516103996896\n\n\n\ny_test_pred = rnd_clf.predict(X_test)\ntest_acc = np.sum(y_test_pred == y_test)/len(y_test)\nprint(\"test accuracy: \"+str(test_acc))\n\ntest accuracy: 0.7704714640198511\n\n\nImmediate improvement by almost 10% with random forest classifier!"
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#model-explanability",
    "href": "posts/2020-10-28-lolpredict.html#model-explanability",
    "title": "LoL Prediction S10 üèπ",
    "section": "Model Explanability",
    "text": "Model Explanability\nLet‚Äôs look at what we were mostly interested. What are some best team compositions!\n\n# import eli5\n# from eli5.sklearn import PermutationImportance\n# perm = PermutationImportance(rnd_clf, random_state=42).fit(X_valid, y_valid)\n# eli5.show_weights(perm, feature_names=X_valid.columns.tolist())\n# Will take billions years to compute"
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#lets-try-shap-summary",
    "href": "posts/2020-10-28-lolpredict.html#lets-try-shap-summary",
    "title": "LoL Prediction S10 üèπ",
    "section": "Let‚Äôs try SHAP summary",
    "text": "Let‚Äôs try SHAP summary\n\nimport shap\n\nexplainer = shap.TreeExplainer(rnd_clf)\nshap_values = explainer.shap_values(X_valid)\nshap.summary_plot(shap_values[1], X_valid)\n\n\n\n\n\n\n\n\n\nWe see that feature 0 (game length) tells us that the game favors blue team winning more when game is shorter which is unexpected. Note that it it not significant at all since SHAP value is -0.02 ~ 0.4 at most.\nGenerally, since all the values are 0 are 1, we can see clear 1-red and 0-blue (When it‚Äôs 0 it has no impact on the prediction)\nWe can see feature 156(blue Mid Akali) helped RED team win more\nWhereas Feature 462(red Top Tryndamere) helps the BLUE team win significantly more haha\nFrom this chart, we can clearly see that each champion has very consistent and predictable contribution to their team‚Äôs chance of winning\n\nNote that - 119 Kindred blue jg - 638 Caitlyn red adc - 60 Renekton blue top - 162 Cassiopeia blue mid - 535 Akali red mid - 376 Thresh blue support - 471 Volibear red top - 31 Jax blue top - 654 Kalista red adc - 290 Miss Fortune blue adc - 259 Ashe blue adc - 360 Rakan blue support - 210 Orianna blue mid - 462 Tryndamere red top - 445 Riven red top - 425 Lucian red top - 715 Janna red support - 156 Akali blue mid - 72 Sylas blue top\nTherefore our best teamp comp impacting positively on winning is ‚Ä¶ - (Top)Renekton/Jax/Sylas (Jg)Kindred (Mid) Cassiopeia/Orianna (Adc)MF/Ashe (Sup)Thresh/Rakan\nMeanwhile worst team comp impacting negatively on winning is ‚Ä¶ - (Top)Volibear/Trynd/Riven/Lucian (Mid)Akali (Adc)Caitlyn/Kalista (Sup)Janna\nWe can also note that Jg role seem to not matter much.. : )\n\n# function to find champ given feature number i\ndef find_champ(i):\n    temp_list = [99,54,101,73,57,96,52,102,68,52]\n    for num in range(len(temp_list)):\n        if (i-temp_list[num] &lt;= 0):\n            return enc.categories_[num][i-1]\n        else:\n            i = i-temp_list[num]\n\n\n# Some things helpful converting sparse matrix into something we can comprehend\n# list_champ = [119, 638,60,162,535,376,471,31,654,290,259,360,210,462,445,425,715,156,72]\n# for champ in list_champ:\n#     lane = ''\n#     if champ &lt;= 99 or 385&lt;=champ&lt;=480:\n#         lane = \"top\"\n#     elif 100 &lt;= champ &lt;=153 or 481&lt;=champ&lt;=532:\n#         lane = \"jg\"\n#     elif 154 &lt;= champ &lt;=255 or 533&lt;=champ&lt;=634:\n#         lane= \"mid\"\n#     elif 256 &lt;= champ &lt;=327 or 635&lt;=champ&lt;=702:\n#         lane= \"adc\"\n#     else:\n#         lane = \"support\"\n#     team = \"blue\" if champ &lt;= 384 else \"red\"\n#     print(champ, find_champ(champ), team, lane)\n#print(len(enc.categories_[0]))    99\n#print(len(enc.categories_[1]))    54   //153\n#print(len(enc.categories_[2]))    101  //254\n#print(len(enc.categories_[3]))    73   //327\n#print(len(enc.categories_[4]))    57  // UP TO 384 is blue team\n#print(len(enc.categories_[5]))    96  //480\n#print(len(enc.categories_[6]))    52  //532\n#print(len(enc.categories_[7]))    102  //634\n#print(len(enc.categories_[8]))    68   //702\n#print(len(enc.categories_[9]))    52   //754"
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#what-if-we-didnt-have-game-length-just-champion-compositions-only",
    "href": "posts/2020-10-28-lolpredict.html#what-if-we-didnt-have-game-length-just-champion-compositions-only",
    "title": "LoL Prediction S10 üèπ",
    "section": "What if we didn‚Äôt have game length, just champion compositions only?",
    "text": "What if we didn‚Äôt have game length, just champion compositions only?\n\nX_1 = sparse_to_df\n\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(X_1,y,test_size=0.2, random_state=42)\n#len(X_train) = 3222\nl = math.floor(3222*0.8)\nX_valid, X_train = X_train_full[:l], X_train_full[l:]\ny_valid, y_train = y_train_full[:l], y_train_full[l:]\nprint(y_valid.shape)\nprint(X_valid.shape)\n\n(2577,)\n(2577, 754)\n\n\n\nrnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\n\nRandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1)\n\n\n\ny_val_pred = rnd_clf.predict(X_valid)\nval_acc = np.sum(y_val_pred == y_valid)/len(y_valid)\nprint(\"validation accuracy: \"+str(val_acc))\n\nvalidation accuracy: 0.7691113698098564\n\n\n\ny_test_pred = rnd_clf.predict(X_test)\ntest_acc = np.sum(y_test_pred == y_test)/len(y_test)\nprint(\"test accuracy: \"+str(test_acc))\n\ntest accuracy: 0.7692307692307693\n\n\nSurprisingly, accuracy only drops less than 0.01. We can conclude that planning out a team comp based on champion‚Äôs strength on early vs late game does not help win more. This can be explained by an example. Let‚Äôs say I picked kayle which is the best late game champion. We may win games with longer duration more but will lose more short games due to her weakness early. So the overall win rate balances out."
  },
  {
    "objectID": "posts/2020-10-28-lolpredict.html#conclusion",
    "href": "posts/2020-10-28-lolpredict.html#conclusion",
    "title": "LoL Prediction S10 üèπ",
    "section": "Conclusion",
    "text": "Conclusion\n\nbest: (Top)Renekton/Jax/Sylas (Jg)Kindred (Mid) Cassiopeia/Orianna (Adc)MF/Ashe (Sup)Thresh/Rakan\nworst: (Top)Volibear/Trynd/Riven/Lucian (Mid)Akali (Adc)Caitlyn/Kalista (Sup)Janna\n\nWe know that in the world of solo queue, picking the above champions will not gurantee a win. Sometimes people are autofilled, meaning they aren‚Äôt playing on their best role. People may disconnect, resulting in games favoring the opposite team. There are too many unknown factors like this, making it impossible to predict 100% of the game outcomes correctly.\nAs a former high elo NA player myself, I can say that generally, the ‚Äòbest team‚Äô above have champions that doesn‚Äôt get countered too often and is a good pick into anything. (This may not be the case for top because I‚Äôve never really cared about top lanes as a support player :). But for ‚Äòworst team‚Äô champions, they are often easily countered. (Especially bottom lane)\nThe biggest surprise was blue team wins more early and red team wins more late (Very slightly but certainly) for some reason. Also jg mattering the least was a surprise as well."
  },
  {
    "objectID": "posts/2020-10-27-city.html",
    "href": "posts/2020-10-27-city.html",
    "title": "City Detector üèôÔ∏è",
    "section": "",
    "text": "Coquitlam Paris Seoul and New York."
  },
  {
    "objectID": "posts/2020-10-27-city.html#lets-make-app-that-recognizes-coquitlam",
    "href": "posts/2020-10-27-city.html#lets-make-app-that-recognizes-coquitlam",
    "title": "City Detector üèôÔ∏è",
    "section": "Let‚Äôs make app that recognizes Coquitlam!",
    "text": "Let‚Äôs make app that recognizes Coquitlam!\n\n#hide\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n#hide\nimport os\nkey = os.environ.get('AZURE_SEARCH_KEY', '39f25aae8d744a528b964a94a4af8b58')"
  },
  {
    "objectID": "posts/2020-10-27-city.html#lets-get-the-images-of-each-city",
    "href": "posts/2020-10-27-city.html#lets-get-the-images-of-each-city",
    "title": "City Detector üèôÔ∏è",
    "section": "Let‚Äôs get the images of each city",
    "text": "Let‚Äôs get the images of each city\n\ncity_types = 'seoul city','coquitlam','paris city', 'new york city'\npath = Path('cities')\n\n\nif not path.exists():\n    path.mkdir()\n    for o in city_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'{o}')\n        download_images(dest, urls=results.attrgot('content_url'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#hide_output\nfns = get_image_files(path)\nfns\n\n(#596) [Path('cities/coquitlam/00000000.jpg'),Path('cities/coquitlam/00000001.jpg'),Path('cities/coquitlam/00000002.png'),Path('cities/coquitlam/00000003.jpg'),Path('cities/coquitlam/00000004.jpg'),Path('cities/coquitlam/00000005.jpg'),Path('cities/coquitlam/00000006.jpg'),Path('cities/coquitlam/00000007.jpg'),Path('cities/coquitlam/00000008.jpg'),Path('cities/coquitlam/00000009.jpg')...]\n\n\n\n#hide_output\nfailed = verify_images(fns)\nfailed\n\n\n\n\n(#16) [Path('cities/coquitlam/00000067.jpg'),Path('cities/coquitlam/00000077.JPG'),Path('cities/coquitlam/00000079.jpg'),Path('cities/coquitlam/00000135.jpg'),Path('cities/new york city/00000010.jpg'),Path('cities/new york city/00000014.jpg'),Path('cities/new york city/00000020.jpg'),Path('cities/new york city/00000026.jpg'),Path('cities/new york city/00000029.jpg'),Path('cities/new york city/00000037.jpg')...]\n\n\n\nfailed.map(Path.unlink)\n\n(#16) [None,None,None,None,None,None,None,None,None,None...]"
  },
  {
    "objectID": "posts/2020-10-27-city.html#lets-load-the-data",
    "href": "posts/2020-10-27-city.html#lets-load-the-data",
    "title": "City Detector üèôÔ∏è",
    "section": "Let‚Äôs load the data",
    "text": "Let‚Äôs load the data\n\nclass DataLoaders(GetAttr):\n    def __init__(self, *loaders): self.loaders = loaders\n    def __getitem__(self, i): return self.loaders[i]\n    train,valid = add_props(lambda i, self: self[i])\n\n\ncities = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = cities.dataloaders(path)\ndls.valid.show_batch(max_n=8, nrows = 2)"
  },
  {
    "objectID": "posts/2020-10-27-city.html#lets-scale-and-augment-the-datas",
    "href": "posts/2020-10-27-city.html#lets-scale-and-augment-the-datas",
    "title": "City Detector üèôÔ∏è",
    "section": "Let‚Äôs scale and augment the datas",
    "text": "Let‚Äôs scale and augment the datas\n\ncities = cities.new(item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                        batch_tfms=aug_transforms())\ndls = cities.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error"
  },
  {
    "objectID": "posts/2020-10-27-city.html#lets-build-and-run-a-cnn-model",
    "href": "posts/2020-10-27-city.html#lets-build-and-run-a-cnn-model",
    "title": "City Detector üèôÔ∏è",
    "section": "Let‚Äôs build and run a CNN model",
    "text": "Let‚Äôs build and run a CNN model\n\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.187395\n1.315027\n0.482759\n00:42\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.307873\n0.871226\n0.336207\n00:41\n\n\n1\n1.064780\n0.831430\n0.241379\n00:41\n\n\n2\n0.876646\n0.767134\n0.215517\n00:41\n\n\n3\n0.784991\n0.738216\n0.224138\n00:49"
  },
  {
    "objectID": "posts/2020-10-27-city.html#lets-look-at-the-confusion-matrix",
    "href": "posts/2020-10-27-city.html#lets-look-at-the-confusion-matrix",
    "title": "City Detector üèôÔ∏è",
    "section": "Let‚Äôs look at the confusion matrix",
    "text": "Let‚Äôs look at the confusion matrix\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\nWe got an accuracy of 26/90 = 71% (rounded)\n\ninterp.plot_top_losses(2,nrows=2)"
  },
  {
    "objectID": "posts/2020-10-27-city.html#lets-try-to-clean-up-the-dataset",
    "href": "posts/2020-10-27-city.html#lets-try-to-clean-up-the-dataset",
    "title": "City Detector üèôÔ∏è",
    "section": "Let‚Äôs try to clean up the dataset",
    "text": "Let‚Äôs try to clean up the dataset\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n# Deleted Photos of maps, inside of home and image of just texts\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete \n\n\ndls = cities.dataloaders(path, num_workers=0)\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(5)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.933403\n1.322945\n0.460870\n00:38\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.258363\n0.800413\n0.347826\n00:37\n\n\n1\n1.014135\n0.660854\n0.243478\n00:38\n\n\n2\n0.851025\n0.609896\n0.243478\n00:38\n\n\n3\n0.725140\n0.591347\n0.217391\n00:37\n\n\n4\n0.623130\n0.582418\n0.226087\n00:37\n\n\n\n\n\nValid_loss doesn‚Äôt decrease so we stop\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(6,nrows=2)\n\n\n\n\n\n\n\n\nIt looks like it has a hard time highlighting seoul city‚Äôs characteristics as most error comes from seoul images. Suspected factors include seoul having new york like buildings, mountains like coquitlam and brick structures like paris city."
  },
  {
    "objectID": "posts/2020-10-27-city.html#ignore-below-deployment-ipr",
    "href": "posts/2020-10-27-city.html#ignore-below-deployment-ipr",
    "title": "City Detector üèôÔ∏è",
    "section": "Ignore Below (Deployment IPR)",
    "text": "Ignore Below (Deployment IPR)\n\nlearn.export()\n\n\n#hide_output\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\n\nlearn_inf = load_learner(path/'export.pkl')\n\n\nlearn_inf.dls.vocab\n\n['coquitlam', 'new york city', 'paris city', 'seoul city']\n\n\n\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\n\n\n\nimg = PILImage.create(btn_upload.data[-1])\n\n\n#hide_output\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\n\n\n\npred,pred_idx,probs = learn_inf.predict(img)\n#hide_output\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred"
  },
  {
    "objectID": "posts/2020-10-27-city.html#classify-button-event-handler",
    "href": "posts/2020-10-27-city.html#classify-button-event-handler",
    "title": "City Detector üèôÔ∏è",
    "section": "Classify Button & Event Handler",
    "text": "Classify Button & Event Handler\n\n#hide_output\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n\n\n\n\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n\n\n#hide\n#Putting back btn_upload to a widget for next cell\nbtn_upload = widgets.FileUpload()\n\n#hide_output\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jaekangai-quarto",
    "section": "",
    "text": "City Detector üèôÔ∏è\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRenekton Croc plush detector üêä\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoL Prediction S10 üèπ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo I learned some new algorithms.. üÉè\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Art Gallery üß†\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMOOC certificates üìú\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoL Prediction S11 üó°Ô∏è\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy friend likes to bike üö¥\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations with IBM üò∫\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJane Street Market EDA üìà\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJane Street Market Prediction üéØ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook Reviews and Storytime üìö\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFairness in Hiring and Salary Statistical Report ‚öñÔ∏è\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-medical Drug Use in Canada üçÅ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 30, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-10-27-croc.html",
    "href": "posts/2020-10-27-croc.html",
    "title": "Renekton Croc plush detector üêä",
    "section": "",
    "text": "Along with my hand drawn arts."
  },
  {
    "objectID": "posts/2020-10-27-croc.html#lets-build-a-crocodile-plush-renekton-detector",
    "href": "posts/2020-10-27-croc.html#lets-build-a-crocodile-plush-renekton-detector",
    "title": "Renekton Croc plush detector üêä",
    "section": "LET‚ÄôS BUILD A CROCODILE, PLUSH, RENEKTON DETECTOR",
    "text": "LET‚ÄôS BUILD A CROCODILE, PLUSH, RENEKTON DETECTOR\n\n#hide\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n\n#hide\nimport os\nkey = os.environ.get('AZURE_SEARCH_KEY', '1a25802a09ab45e4a082267f88ee5bd1')\n\n\nreptile_types = 'crocodile','alligator plush', 'renekton'\npath = Path('reptiles')\n\n\nif not path.exists():\n    path.mkdir()\n    for o in reptile_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_bing(key, f'{o}')\n        download_images(dest, urls=results.attrgot('content_url'))\n\n\n\n\n\n\n\n\n\n\n\nfns = get_image_files(path)\nfns\n\n(#416) [Path('reptiles/alligator plush/00000000.jpg'),Path('reptiles/alligator plush/00000001.jpg'),Path('reptiles/alligator plush/00000002.jpg'),Path('reptiles/alligator plush/00000003.jpg'),Path('reptiles/alligator plush/00000004.jpeg'),Path('reptiles/alligator plush/00000005.jpg'),Path('reptiles/alligator plush/00000006.jpg'),Path('reptiles/alligator plush/00000007.jpg'),Path('reptiles/alligator plush/00000008.jpg'),Path('reptiles/alligator plush/00000009.jpg')...]\n\n\n\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\n\n\n\n\n(#0) []\n\n\n\nclass DataLoaders(GetAttr):\n    num_workers=0\n    def __init__(self, *loaders): self.loaders = loaders\n    def __getitem__(self, i): return self.loaders[i]\n    train,valid = add_props(lambda i, self: self[i])\n        \n\n\nreptiles = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = reptiles.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\n\nreptiles = reptiles.new(item_tfms=RandomResizedCrop(224, min_scale=0.5),\n                        batch_tfms=aug_transforms())\ndls = reptiles.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error\n\n\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.455003\n0.249432\n0.084337\n00:18\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.104521\n0.049535\n0.024096\n00:18\n\n\n1\n0.068319\n0.012980\n0.012048\n00:18\n\n\n2\n0.052283\n0.011862\n0.000000\n00:19\n\n\n3\n0.041685\n0.010840\n0.000000\n00:19\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\nResult is very good\n\n# cleaner = ImageClassifierCleaner(learn)\n# cleaner\n# for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete"
  },
  {
    "objectID": "posts/2020-10-27-croc.html#lets-test",
    "href": "posts/2020-10-27-croc.html#lets-test",
    "title": "Renekton Croc plush detector üêä",
    "section": "Let‚Äôs test",
    "text": "Let‚Äôs test\n\nmy_renek = PILImage.create(\"renek_plush.png\")\ndisplay(my_renek.to_thumb(256,256))\n\n\n\n\n\n\n\n\n\npred, pred_idx, probs =learn.predict(my_renek)\nf'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\n\n\n\n'Prediction: alligator plush; Probability: 0.9391'\n\n\nVery good. It is very accurate since my drawing of a plush is very realistic.\n\nrenek = PILImage.create(\"renek_test.png\")\ndisplay(renek.to_thumb(256,256))\npred, pred_idx, probs =learn.predict(renek)\nf'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\n\n\n\n\n\n\n\n\n\n\n'Prediction: renekton; Probability: 0.9834'\n\n\nEasily recognizes my drawing of Renekton as well. I guess I‚Äôm an artist\n\nrenek_withoutbg = PILImage.create(\"renek_test1.png\")\ndisplay(renek_withoutbg.to_thumb(256,256))\npred, pred_idx, probs =learn.predict(renek_withoutbg)\nf'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\n\n\n\n\n\n\n\n\n\n\n'Prediction: renekton; Probability: 0.9674'\n\n\nExpected the model to predict plush becasue I removed the background but it‚Äôs too smart. (In dataset a lot of plush had empty white background contrast to lots of Renekton images having dark backgrounds)\n\nbeard = PILImage.create(\"beard.jpg\")\ndisplay(beard.to_thumb(200,200))\npred, pred_idx, probs =learn.predict(beard)\nf'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\n\n\n\n\n\n\n\n\n\n\n'Prediction: alligator plush; Probability: 0.8644'\n\n\nIndeed I am an alligator plush with my fake beard!\n\nlearn.export()"
  },
  {
    "objectID": "posts/2020-10-27-croc.html#run-code-below-to-make-your-own-test-download-export.pkl-file-on-my-github",
    "href": "posts/2020-10-27-croc.html#run-code-below-to-make-your-own-test-download-export.pkl-file-on-my-github",
    "title": "Renekton Croc plush detector üêä",
    "section": "RUN CODE BELOW TO MAKE YOUR OWN TEST (Download export.pkl file on my github)",
    "text": "RUN CODE BELOW TO MAKE YOUR OWN TEST (Download export.pkl file on my github)\n\nfrom fastai.vision.widgets import *\nbtn_upload = widgets.FileUpload()\nout_pl = widgets.Output()\nlbl_pred = widgets.Label()\n\n\npath = Path('')\nlearn_inf = load_learner(path/'export.pkl', cpu=True)\n\n\ndef on_data_change(change):\n    lbl_pred.value = ''\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\n\nbtn_upload.observe(on_data_change, names=['data'])\n\ndisplay(VBox([widgets.Label('Feed me a reptile photo!'), btn_upload, out_pl, lbl_pred]))"
  },
  {
    "objectID": "posts/2020-11-27-So-I-learned-some-new-algorithms.html",
    "href": "posts/2020-11-27-So-I-learned-some-new-algorithms.html",
    "title": "So I learned some new algorithms.. üÉè",
    "section": "",
    "text": "Learning with memes\n\n\ntoc: true\nbranch: master\nbadges: true\ncomments: true\nauthor: Jaekang Lee\ncategories: [fastpages, jupyter,meme]\n\n##\n\nIntroduction\n\n\n#hide_input\nintroduction = PILImage.create(\"AlgoMeme/introduction.jpeg\")\ndisplay(renek.to_thumb(500,500))\n\n\n\n\n\n\n\n\n\n#hide_input\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\n##\n\nInception Net\n\n\n#hide_input\ninception = PILImage.create(\"AlgoMeme/inceptionNet.jpg\")\ndisplay(inception.to_thumb(400,500))\n\n\n\n\n\n\n\n\n##\n\nYou only look once algorithm\n\n####\n\nHow do you detect multiple cars in a single image?\n\n\n#hide_input\nyolo = PILImage.create(\"AlgoMeme/yolo.png\")\ndisplay(yolo.to_thumb(400,500))\n\n\n\n\n\n\n\n\n##\n\nObject Detection\n\n\n#hide_input\ndetection = PILImage.create(\"AlgoMeme/detection.png\")\ndisplay(detection.to_thumb(600,800))"
  },
  {
    "objectID": "posts/2020-12-30-certificates.html",
    "href": "posts/2020-12-30-certificates.html",
    "title": "MOOC certificates üìú",
    "section": "",
    "text": "certificates\n\n\ntoc: true\nbranch: master\nbadges: true\ncomments: true\nauthor: Jaekang Lee\ncategories: [fastpages, jupyter,certificate]\n\n\nUdacity\n\nData Scientist Nanodegree\n\n\n\nCoursera\n\nGenerative Adversarial Networks (GANs) Specialization\nApply Generative Adversarial Networks (GANs)\nBuild Better Generative Adversarial Networks (GANs)\nBuild Basic Generative Adversarial Networks (GANs)\nStandford Machine Learning (unofficial)\nConvolutional Neural Networks\nSequence Models\nNeural Networks and Deep Learning\n\n\n\nKaggle\n\nDeep Learning\nFeature Engineering\nIntro to Machine Learning\nIntermediate Machine Learning\nMachine Learning Explainability\nNatural Language Processing\n\n\n\nUdemy\n\nTableau 20 Advanced Training: Master Tableau in Data Science\nTableau 2020 A-Z: Hands-On Tableau Training for Data Science\nScala and Spark for Big Data and Machine Learning"
  },
  {
    "objectID": "posts/2021-01-05-bike.html",
    "href": "posts/2021-01-05-bike.html",
    "title": "My friend likes to bike üö¥",
    "section": "",
    "text": "Building interactive webapp with Python, Bootstrap and Flask\n\n\ntoc: true\nbadges: true\ncomments: true\nauthor: Jaekang Lee\nimage: images/janna.jpg\ncategories: [python, jupyter, CRISP-DM, bootstrap, plotly, flask, pca]\n\n\n#hide_input\nfrom IPython.display import Image\nImage(filename = \"images/dash.png\")\n\n\n\n\n\n\n\n\n\n#hide_input\nImage(filename = \"images/dash2.png\")\n\n\n\n\n\n\n\n\n\nWEB-APP HERE\nNote that the web-app takes about a min to load! # GIT REPOSITORY HERE\nThis is just codes I used on the notebook to understand and clean the data. The data comes from my friend who likes to bike.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nimport plotly.graph_objs as go\nimport plotly.express as px\n%matplotlib inline\n\n\ndf = pd.read_csv ('data/activities.csv')\norig = pd.read_csv('data/activities.csv')\ndf = shuffle(df)\nprint(\"number of rows: \"+ str(len(df)))\ndf.head(1)\n\nnumber of rows: 142\n\n\n\n\n\n\n\n\n\nActivity ID\nActivity Date\nActivity Name\nActivity Type\nActivity Description\nElapsed Time\nDistance\nRelative Effort\nCommute\nActivity Gear\n...\nGear\nPrecipitation Probability\nPrecipitation Type\nCloud Cover\nWeather Visibility\nUV Index\nWeather Ozone\ntranslation missing: en-US.lib.export.portability_exporter.activities.horton_values.jump_count\ntranslation missing: en-US.lib.export.portability_exporter.activities.horton_values.total_grit\ntranslation missing: en-US.lib.export.portability_exporter.activities.horton_values.avg_flow\n\n\n\n\n47\n723876967\nSep 24, 2016, 10:54:54 PM\nAfternoon Ride\nRide\nNaN\n12735\n29.55\nNaN\nFalse\nVilano Aluminum Road Bike 21 Speed Shimano\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1 rows √ó 77 columns\n\n\n\n\n# Find all columns that have at least 75% non null values\nquater_nulls = list(df.columns[df.isnull().sum() &lt;= 0.25*len(df)])\n#quater_nulls\n\n\n# Drop too many null columns\ndf = df[quater_nulls]\ndf.columns\n\nIndex(['Activity ID', 'Activity Date', 'Activity Name', 'Activity Type',\n       'Elapsed Time', 'Distance', 'Commute', 'Activity Gear', 'Filename',\n       'Athlete Weight', 'Bike Weight', 'Elapsed Time.1', 'Moving Time',\n       'Distance.1', 'Max Speed', 'Elevation Gain', 'Elevation Low',\n       'Elevation High', 'Max Grade', 'Average Grade', 'Average Watts',\n       'Calories', 'Commute.1', 'Bike'],\n      dtype='object')\n\n\nGoing to remove discard = [‚ÄòActivity Name‚Äô, ‚ÄòActivity ID‚Äô, ‚ÄòCommute‚Äô, ‚ÄòFilename‚Äô, ‚ÄòCommute.1‚Äô,‚ÄòDistance‚Äô, ‚ÄòElapsed Time.1‚Äô, ‚ÄòBike‚Äô]  Because not information or repetitive.\n\ndiscard = ['Activity Name', 'Activity ID', 'Commute', 'Filename', 'Commute.1','Distance', 'Elapsed Time.1', 'Bike']\ndf = df.drop(discard, axis = 1)\ndf.head(1)\n\n\n\n\n\n\n\n\nActivity Date\nActivity Type\nElapsed Time\nActivity Gear\nAthlete Weight\nBike Weight\nMoving Time\nDistance.1\nMax Speed\nElevation Gain\nElevation Low\nElevation High\nMax Grade\nAverage Grade\nAverage Watts\nCalories\n\n\n\n\n47\nSep 24, 2016, 10:54:54 PM\nRide\n12735\nVilano Aluminum Road Bike 21 Speed Shimano\n63.502899\n11.0\n7946.0\n29549.900391\n12.3\n11.1737\n1.2\n13.2\n38.299999\n-0.002369\n53.709999\n475.859314\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nElapsed Time\nAthlete Weight\nBike Weight\nMoving Time\nDistance.1\nMax Speed\nElevation Gain\nElevation Low\nElevation High\nMax Grade\nAverage Grade\nAverage Watts\nCalories\n\n\n\n\ncount\n142.000000\n128.000000\n121.000000\n142.000000\n142.000000\n136.000000\n137.000000\n135.000000\n135.000000\n136.000000\n142.000000\n126.000000\n132.000000\n\n\nmean\n8307.028169\n65.011994\n8.886777\n5964.485915\n34740.822462\n13.735294\n262.614366\n26.635556\n95.854814\n27.179412\n0.271421\n111.032301\n764.836008\n\n\nstd\n5371.122774\n5.263799\n1.400711\n3368.764442\n21521.125565\n3.903418\n270.558590\n47.756425\n103.870238\n15.398105\n3.187175\n28.786978\n489.682759\n\n\nmin\n204.000000\n55.000000\n7.500000\n182.000000\n0.000000\n0.000000\n0.000000\n-18.000000\n6.900000\n0.000000\n-0.752807\n49.716900\n26.208254\n\n\n25%\n3414.500000\n60.000000\n7.500000\n2949.000000\n17527.250488\n11.800000\n62.490898\n-1.000000\n21.850000\n14.350000\n-0.003579\n91.731985\n383.654442\n\n\n50%\n8070.500000\n68.000000\n9.000000\n6047.500000\n31560.699219\n13.700000\n166.636993\n0.900000\n101.099998\n22.300000\n0.000000\n114.522282\n673.522827\n\n\n75%\n11301.000000\n68.000000\n11.000000\n7957.250000\n50011.325195\n15.300000\n358.088989\n72.400002\n125.799999\n42.899999\n0.010629\n130.717503\n1066.323883\n\n\nmax\n28317.000000\n70.000000\n11.000000\n16708.000000\n91705.296875\n36.299999\n1455.640015\n382.299988\n1092.099976\n50.000000\n37.947071\n182.307999\n2375.330322\n\n\n\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True, fmt='.2f', ax = plt.figure(figsize = (15,10)).gca())\n\n\n\n\n\n\n\n\nInteresting Correlations: - Elevation Low,High with Athelete Weight, bike weight - Calories and Moving Time and Distance.1 and Elevation Gain - Elevation High, Low and Average Grade - Elevation Gain with Elapsed time, Moving Time, Distance\n\ndf.hist(ax = plt.figure(figsize = (15,20)).gca());\n\nUserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  df.hist(ax = plt.figure(figsize = (15,20)).gca());\n\n\n\n\n\n\n\n\n\n\nCategorical Columns\n\ncat_df = df.select_dtypes(include=['object']) #choose categorical columns\ncat_df\n\n\n\n\n\n\n\n\nActivity Date\nActivity Type\nActivity Gear\n\n\n\n\n47\nSep 24, 2016, 10:54:54 PM\nRide\nVilano Aluminum Road Bike 21 Speed Shimano\n\n\n8\nMay 23, 2015, 10:26:06 PM\nRide\nNaN\n\n\n94\nSep 29, 2018, 4:19:38 PM\nRide\nGusto\n\n\n55\nMar 22, 2017, 3:44:50 PM\nRide\nKestrel 200 SCI Older Road Bike\n\n\n40\nApr 16, 2016, 5:57:42 PM\nRide\nVilano Aluminum Road Bike 21 Speed Shimano\n\n\n...\n...\n...\n...\n\n\n30\nMar 16, 2016, 6:25:36 PM\nRide\nVilano Aluminum Road Bike 21 Speed Shimano\n\n\n66\nJun 23, 2017, 11:25:10 PM\nRide\nKestrel 200 SCI Older Road Bike\n\n\n62\nMay 9, 2017, 10:33:30 PM\nRide\nKestrel 200 SCI Older Road Bike\n\n\n91\nAug 22, 2018, 9:34:35 PM\nRide\nGusto\n\n\n35\nMar 23, 2016, 5:35:32 AM\nRun\nNaN\n\n\n\n\n142 rows √ó 3 columns\n\n\n\nLets clean these up üßπ\n\n# Fix Date column into Year, Month, Day, Hour\ntime = df['Activity Date'].astype('datetime64[ns]')\nyr,mon,d,h = [],[],[],[]\nfor i in time:\n    yr.append(i.year)\n    mon.append(i.month)\n    d.append(i.day)\n    h.append(i.hour)\nlen(yr)\ntime.head(4)\ndf['Year'] = yr\ndf['Month'] = mon\ndf['Day'] = d\ndf['Hour'] = h\n\n\ndf = df.drop(['Activity Date'], axis=1) # Drop original Date value\ndf.head(3)    \n\n\n\n\n\n\n\n\nActivity Type\nElapsed Time\nActivity Gear\nAthlete Weight\nBike Weight\nMoving Time\nDistance.1\nMax Speed\nElevation Gain\nElevation Low\nElevation High\nMax Grade\nAverage Grade\nAverage Watts\nCalories\nYear\nMonth\nDay\nHour\n\n\n\n\n47\nRide\n12735\nVilano Aluminum Road Bike 21 Speed Shimano\n63.502899\n11.0\n7946.0\n29549.900391\n12.3\n11.173700\n1.2\n13.200000\n38.299999\n-0.002369\n53.709999\n475.859314\n2016\n9\n24\n22\n\n\n8\nRide\n11734\nNaN\n56.699001\nNaN\n10057.0\n59956.300781\n14.6\n825.666992\n-2.4\n101.099998\n46.500000\n0.079558\n130.302002\n1461.148682\n2015\n5\n23\n22\n\n\n94\nRide\n4696\nGusto\n68.000000\n7.5\n4127.0\n27227.500000\n14.0\n158.414581\n75.0\n158.199997\n11.000000\n0.235424\n109.483162\n580.913513\n2018\n9\n29\n16\n\n\n\n\n\n\n\n\nprint(\"Unique Activity Gear values: \" + str(df['Activity Gear'].unique()))\nprint(\"Unique Activity Gear values: \" + str(df['Activity Type'].unique()))\n\nUnique Activity Gear values: ['Gusto' 'Kestrel 200 SCI Older Road Bike' nan\n 'Vilano Aluminum Road Bike 21 Speed Shimano' 'Fixie']\nUnique Activity Gear values: ['Ride' 'Hike' 'Run' 'Workout' 'Walk']\n\n\n\n# Function to create dummy variables\ndef create_dummy_df(df, cat_cols, dummy_na=False):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    cat_cols - list of strings that are associated with names of the categorical columns\n    dummy_na - Bool whether you want to dummy NA values or not\n    \n    OUTPUT:\n    df - new dataframe with following characteristics:\n        1. contains all columns that were not specified as categorical\n        2. removes all the original columns in cat_cols\n        3. dummy columns for each of the categorical columns in cat_cols\n        4. use a prefix of the column name with an underscore (_) for separating\n        5. if dummy_na is True - it also contains dummy columns for NaN values\n    '''\n    for col in cat_cols:\n        try:\n            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1)\n        except:\n            continue\n    return df\n\ndf = create_dummy_df(df, ['Activity Type'], dummy_na = True)\ndf.head(3)\n\n\n\n\n\n\n\n\nElapsed Time\nActivity Gear\nAthlete Weight\nBike Weight\nMoving Time\nDistance.1\nMax Speed\nElevation Gain\nElevation Low\nElevation High\n...\nCalories\nYear\nMonth\nDay\nHour\nActivity Type_Ride\nActivity Type_Run\nActivity Type_Walk\nActivity Type_Workout\nActivity Type_nan\n\n\n\n\n47\n12735\nVilano Aluminum Road Bike 21 Speed Shimano\n63.502899\n11.0\n7946.0\n29549.900391\n12.3\n11.173700\n1.2\n13.200000\n...\n475.859314\n2016\n9\n24\n22\n1\n0\n0\n0\n0\n\n\n8\n11734\nNaN\n56.699001\nNaN\n10057.0\n59956.300781\n14.6\n825.666992\n-2.4\n101.099998\n...\n1461.148682\n2015\n5\n23\n22\n1\n0\n0\n0\n0\n\n\n94\n4696\nGusto\n68.000000\n7.5\n4127.0\n27227.500000\n14.0\n158.414581\n75.0\n158.199997\n...\n580.913513\n2018\n9\n29\n16\n1\n0\n0\n0\n0\n\n\n\n\n3 rows √ó 23 columns\n\n\n\n\n\nNull Values\n\nno_nulls = list(df.columns[df.isnull().sum() != 0])\nno_nulls\n\n['Activity Gear',\n 'Athlete Weight',\n 'Bike Weight',\n 'Max Speed',\n 'Elevation Gain',\n 'Elevation Low',\n 'Elevation High',\n 'Max Grade',\n 'Average Watts',\n 'Calories']\n\n\nBased on their histogram, it seem like a good idea to  - Imputation on median: [Athlete Weight, Bike Weight, Elevation Low, Elevation High]  - Imputation on mean: [Elevation Gain, Average Watts, Calories, Max Speed, Max Grade]\n\n# Imputation functions\nfill_mean = lambda col: col.fillna(col.mean()) # function for imputating mean\nfill_median = lambda col: col.fillna(col.median()) # function for imputating median\n\n# impuation on mean\nfill_df = df[['Elevation Gain', 'Average Watts', 'Calories', 'Max Speed', 'Max Grade']].apply(fill_mean, axis=0) \nfill_df = pd.concat([fill_df, df.drop(['Elevation Gain', 'Average Watts', 'Calories', 'Max Speed', 'Max Grade'], axis=1)], axis=1)\n# imputation on median\nfill_df_med = df[['Athlete Weight', 'Bike Weight', 'Elevation Low', 'Elevation High']].apply(fill_median, axis=0)\nfilled_df = pd.concat([fill_df.drop(['Athlete Weight', 'Bike Weight', 'Elevation Low', 'Elevation High'], axis = 1), fill_df_med], axis=1)\n# Alternative solution to null values by dropping all\ndropped_df = df.dropna()\nfilled_df.head(2)\n\n\n\n\n\n\n\n\nElevation Gain\nAverage Watts\nCalories\nMax Speed\nMax Grade\nElapsed Time\nActivity Gear\nMoving Time\nDistance.1\nAverage Grade\n...\nHour\nActivity Type_Ride\nActivity Type_Run\nActivity Type_Walk\nActivity Type_Workout\nActivity Type_nan\nAthlete Weight\nBike Weight\nElevation Low\nElevation High\n\n\n\n\n8\n825.666992\n130.302002\n1461.148682\n14.600000\n46.500000\n11734\nNaN\n10057.0\n59956.300781\n0.079558\n...\n22\n1\n0\n0\n0\n0\n56.699001\n9.0\n-2.4\n101.099998\n\n\n28\n41.823601\n128.156006\n1058.558350\n19.200001\n24.700001\n8448\nVilano Aluminum Road Bike 21 Speed Shimano\n7408.0\n53329.601562\n-0.015939\n...\n15\n1\n0\n0\n0\n0\n60.000000\n11.0\n-1.0\n42.200001\n\n\n\n\n2 rows √ó 23 columns\n\n\n\n\ndropped_df.head(2)\n\n\n\n\n\n\n\n\nElapsed Time\nActivity Gear\nAthlete Weight\nBike Weight\nMoving Time\nDistance.1\nMax Speed\nElevation Gain\nElevation Low\nElevation High\n...\nCalories\nYear\nMonth\nDay\nHour\nActivity Type_Ride\nActivity Type_Run\nActivity Type_Walk\nActivity Type_Workout\nActivity Type_nan\n\n\n\n\n101\n11327\nGusto\n68.000000\n7.5\n7993.0\n54209.500000\n11.5\n240.664948\n74.800003\n113.900002\n...\n975.611206\n2019\n5\n15\n21\n1\n0\n0\n0\n0\n\n\n44\n5335\nVilano Aluminum Road Bike 21 Speed Shimano\n67.131599\n11.0\n5038.0\n38688.300781\n11.8\n24.196800\n0.000000\n13.900000\n...\n770.871704\n2016\n8\n29\n17\n1\n0\n0\n0\n0\n\n\n\n\n2 rows √ó 23 columns\n\n\n\n\nfilled_df = filled_df.dropna() #Note: can change this to na -&gt; no bike (on foot)\n\n\nno_nulls = list(filled_df.columns[filled_df.isnull().sum() != 0])\nno_nulls_2 = list(dropped_df.columns[dropped_df.isnull().sum() != 0])\nassert(no_nulls_2 == [])\nassert(no_nulls == [])\n\n\n\nLinear Regression\n\n# Split data PREDICT DISTANCE\ny = filled_df['Distance.1']\nX = filled_df.drop(['Distance.1'], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=42)\n\nlm_model = LinearRegression(normalize=True)\n\n\nlm_model.fit(X_train, y_train)\n\nLinearRegression(normalize=True)\n\n\n\n# R-Squared Score\ntest_pred = lm_model.predict(X_test)\ntrain_pred = lm_model.predict(X_train)\nr2_test = r2_score(y_test, test_pred)\nr2_train = r2_score(y_train, train_pred)\nprint(\"test r2: \"+str(r2_test))\nprint(\"train r2: \"+str(r2_train))\n\ntest r2: 0.6550060428615112\ntrain r2: 0.9398003894033616\n\n\n\n# Split data PREDICT BIKE\nbike_df = filled_df.drop(['Bike Weight'], axis=1) # Drop bike weight so it doesn't cheat\ny = bike_df['Activity Gear']\nX = bike_df.drop(['Activity Gear'], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=42)\n\n\n\nRandom Forests\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\n\nRandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1)\n\n\n\ny_test_pred = rnd_clf.predict(X_test)\ntest_acc = np.sum(y_test_pred == y_test)/len(y_test)\nprint(\"test accuracy: \"+str(test_acc))\n\ntest accuracy: 0.8421052631578947\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n#https://stackoverflow.com/questions/65549588/shap-treeexplainer-for-randomforest-multiclass-what-is-shap-valuesi\n\nlabels = [\n    \"Fixie\",\n    \"Kestrel 200 SCI Older Road Bike\",\n    \"Vilano Aluminum Road Bike 21 Speed Shimano\",\n    \"Gusto\",\n]\nle = LabelEncoder()\nz = le.fit_transform(labels)\nencoding_scheme = dict(zip(z, labels))\nprint(encoding_scheme)\n\n{0: 'Fixie', 2: 'Kestrel 200 SCI Older Road Bike', 3: 'Vilano Aluminum Road Bike 21 Speed Shimano', 1: 'Gusto'}\n\n\n\nsum(y == 'Kestrel 200 SCI Older Road Bike')\n\n39\n\n\n\nimport shap\n\nexplainer = shap.TreeExplainer(rnd_clf)\nshap_values = explainer.shap_values(X)\n# SHAP plot for Gusto\nshap.summary_plot(shap_values[1], X)\n\n\n\n\n\n\n\n\n\n# SHAP plot for Kestrel 200 SCI Older Road Bike\nshap.summary_plot(shap_values[2], X)\n\n\n\n\n\n\n\n\n\n# SHAP plot for Vilano Aluminum Road Bike 21 Speed Shimano\nshap.summary_plot(shap_values[3], X)\n\n\n\n\n\n\n\n\n\n\nMake it harder for computer to guess\n\n# Let us make it even harder for the computer to guess\nbike_df = filled_df.drop(['Bike Weight', 'Year', 'Athlete Weight'], axis=1) # Drop bike weight so it doesn't cheat\ny = bike_df['Activity Gear']\nX = bike_df.drop(['Activity Gear'], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=42)\n\n\nrnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\n\nRandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1)\n\n\n\ny_test_pred = rnd_clf.predict(X_test)\ntest_acc = np.sum(y_test_pred == y_test)/len(y_test)\nprint(\"test accuracy: \"+str(test_acc))\n\ntest accuracy: 0.5263157894736842\n\n\n\nexplainer = shap.TreeExplainer(rnd_clf)\nshap_values = explainer.shap_values(X)\n# Gustov\nshap.summary_plot(shap_values[1], X)\n\n\n\n\n\n\n\n\n\nshap.dependence_plot('Max Grade', shap_values[1], X, interaction_index='Elevation Low')\n\n\n\n\n\n\n\n\nWhat the heck makes Elevation Low a good guessing tool? Maybe my friend liked more mountains with certain bikes\n\n#hide-output\n# Plotly tests\nimport plotly.express as px\n\ndf_i = px.data.iris()\nfeatures = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\n\nfig = px.scatter_matrix(\n    df_i,\n    dimensions=features,\n    color=\"species\"\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()"
  },
  {
    "objectID": "posts/2021-01-23-JaneStreet-Copy1.html",
    "href": "posts/2021-01-23-JaneStreet-Copy1.html",
    "title": "Jane Street Market EDA üìà",
    "section": "",
    "text": "Jane Street Market Prediction Kaggle Competition\n#hide_input\nImage(filename = \"images/jane_logo.png\", width =800, height = 300)"
  },
  {
    "objectID": "posts/2021-01-23-JaneStreet-Copy1.html#eda-and-visualization",
    "href": "posts/2021-01-23-JaneStreet-Copy1.html#eda-and-visualization",
    "title": "Jane Street Market EDA üìà",
    "section": "EDA and Visualization",
    "text": "EDA and Visualization\n\n#hide_input\n#!pip install datatable &gt; /dev/null\nimport datatable as dt\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.gridspec as gridspec\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom collections import defaultdict\n# garbage collector to keep RAM in check\nimport gc \n%matplotlib inline\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n\nImport Data üìö\n\n# df = pd.read_csv(\"../input/jane-street-market-prediction/train.csv\")\ndf = dt.fread('../../../Kaggle/Jane-Street-Market-Prediction/input/train.csv')\ndf = df.to_pandas()\n\n\nfeat = pd.read_csv(\"../../../Kaggle/Jane-Street-Market-Prediction/input/features.csv\")\n\n\n#hide_input\nprint(\"df.shape: \" + str(df.shape))\nprint(\"how many days? \" + str(len(df.date.unique())) + \"days\")\ndf.head()\n\ndf.shape: (2390491, 138)\nhow many days? 500days\n\n\n\n\n\n\n\n\n\ndate\nweight\nresp_1\nresp_2\nresp_3\nresp_4\nresp\nfeature_0\nfeature_1\nfeature_2\n...\nfeature_121\nfeature_122\nfeature_123\nfeature_124\nfeature_125\nfeature_126\nfeature_127\nfeature_128\nfeature_129\nts_id\n\n\n\n\n0\n0\n0.000000\n0.009916\n0.014079\n0.008773\n0.001390\n0.006270\n1\n-1.872746\n-2.191242\n...\nNaN\n1.168391\n8.313583\n1.782433\n14.018213\n2.653056\n12.600292\n2.301488\n11.445807\n0\n\n\n1\n0\n16.673515\n-0.002828\n-0.003226\n-0.007319\n-0.011114\n-0.009792\n-1\n-1.349537\n-1.704709\n...\nNaN\n-1.178850\n1.777472\n-0.915458\n2.831612\n-1.417010\n2.297459\n-1.304614\n1.898684\n1\n\n\n2\n0\n0.000000\n0.025134\n0.027607\n0.033406\n0.034380\n0.023970\n-1\n0.812780\n-0.256156\n...\nNaN\n6.115747\n9.667908\n5.542871\n11.671595\n7.281757\n10.060014\n6.638248\n9.427299\n2\n\n\n3\n0\n0.000000\n-0.004730\n-0.003273\n-0.000461\n-0.000476\n-0.003200\n-1\n1.174378\n0.344640\n...\nNaN\n2.838853\n0.499251\n3.033732\n1.513488\n4.397532\n1.266037\n3.856384\n1.013469\n3\n\n\n4\n0\n0.138531\n0.001252\n0.002165\n-0.001215\n-0.006219\n-0.002604\n1\n-3.172026\n-3.093182\n...\nNaN\n0.344850\n4.101145\n0.614252\n6.623456\n0.800129\n5.233243\n0.362636\n3.926633\n4\n\n\n\n\n5 rows √ó 138 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ndate\nweight\nresp_1\nresp_2\nresp_3\nresp_4\nresp\nfeature_0\nfeature_1\nfeature_2\n...\nfeature_121\nfeature_122\nfeature_123\nfeature_124\nfeature_125\nfeature_126\nfeature_127\nfeature_128\nfeature_129\nts_id\n\n\n\n\ncount\n2.390491e+06\n2.390491e+06\n2.390491e+06\n2.390491e+06\n2.390491e+06\n2.390491e+06\n2.390491e+06\n2.390491e+06\n2.390491e+06\n2.390491e+06\n...\n2.320637e+06\n2.390268e+06\n2.390268e+06\n2.374408e+06\n2.374408e+06\n2.381638e+06\n2.381638e+06\n2.388570e+06\n2.388570e+06\n2.390491e+06\n\n\nmean\n2.478668e+02\n3.031535e+00\n1.434969e-04\n1.980749e-04\n2.824183e-04\n4.350201e-04\n4.083113e-04\n9.838565e-03\n3.855776e-01\n3.576875e-01\n...\n2.687757e-01\n3.435523e-01\n2.799973e-01\n3.351537e-01\n2.448752e-01\n3.391778e-01\n2.323809e-01\n3.425608e-01\n2.456182e-01\n1.195245e+06\n\n\nstd\n1.522746e+02\n7.672794e+00\n8.930163e-03\n1.230236e-02\n1.906882e-02\n3.291224e-02\n2.693609e-02\n9.999518e-01\n2.559373e+00\n2.477335e+00\n...\n2.174238e+00\n2.087842e+00\n1.977643e+00\n1.742587e+00\n2.242853e+00\n2.534498e+00\n1.795854e+00\n2.307130e+00\n1.765419e+00\n6.900755e+05\n\n\nmin\n0.000000e+00\n0.000000e+00\n-3.675043e-01\n-5.328334e-01\n-5.681196e-01\n-5.987447e-01\n-5.493845e-01\n-1.000000e+00\n-3.172026e+00\n-3.093182e+00\n...\n-7.471971e+00\n-5.862979e+00\n-6.029281e+00\n-4.080720e+00\n-8.136407e+00\n-8.215050e+00\n-5.765982e+00\n-7.024909e+00\n-5.282181e+00\n0.000000e+00\n\n\n25%\n1.040000e+02\n1.617400e-01\n-1.859162e-03\n-2.655044e-03\n-5.030704e-03\n-9.310415e-03\n-7.157903e-03\n-1.000000e+00\n-1.299334e+00\n-1.263628e+00\n...\n-1.123252e+00\n-1.114326e+00\n-9.512009e-01\n-9.133750e-01\n-1.212124e+00\n-1.452912e+00\n-8.993050e-01\n-1.278341e+00\n-8.544535e-01\n5.976225e+05\n\n\n50%\n2.540000e+02\n7.086770e-01\n4.552665e-05\n6.928179e-05\n1.164734e-04\n1.222579e-04\n8.634997e-05\n1.000000e+00\n-1.870182e-05\n-7.200577e-07\n...\n0.000000e+00\n7.006244e-17\n6.054629e-17\n4.870826e-17\n-2.558675e-16\n1.015055e-16\n5.419920e-17\n8.563069e-17\n4.869529e-17\n1.195245e+06\n\n\n75%\n3.820000e+02\n2.471791e+00\n2.097469e-03\n2.939111e-03\n5.466336e-03\n9.804649e-03\n7.544347e-03\n1.000000e+00\n1.578417e+00\n1.526399e+00\n...\n1.342829e+00\n1.405926e+00\n1.308625e+00\n1.228277e+00\n1.409687e+00\n1.767275e+00\n1.111491e+00\n1.582633e+00\n1.125321e+00\n1.792868e+06\n\n\nmax\n4.990000e+02\n1.672937e+02\n2.453477e-01\n2.949339e-01\n3.265597e-01\n5.113795e-01\n4.484616e-01\n1.000000e+00\n7.442989e+01\n1.480763e+02\n...\n1.107771e+02\n4.812516e+01\n1.276908e+02\n6.514517e+01\n7.052807e+01\n5.872849e+01\n6.932221e+01\n5.119038e+01\n1.164568e+02\n2.390490e+06\n\n\n\n\n8 rows √ó 138 columns\n\n\n\n\nfeat.describe()\n\n\n\n\n\n\n\n\nfeature\ntag_0\ntag_1\ntag_2\ntag_3\ntag_4\ntag_5\ntag_6\ntag_7\ntag_8\n...\ntag_19\ntag_20\ntag_21\ntag_22\ntag_23\ntag_24\ntag_25\ntag_26\ntag_27\ntag_28\n\n\n\n\ncount\n130\n130\n130\n130\n130\n130\n130\n130\n130\n130\n...\n130\n130\n130\n130\n130\n130\n130\n130\n130\n130\n\n\nunique\n130\n2\n2\n2\n2\n2\n2\n2\n2\n2\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\ntop\nfeature_25\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\nfreq\n1\n113\n113\n113\n113\n113\n122\n90\n128\n128\n...\n123\n125\n125\n121\n82\n118\n118\n118\n118\n118\n\n\n\n\n4 rows √ó 30 columns\n\n\n\nAs told, all the featues and even tags are anonymized. There‚Äôs not much human interpretability just from describe tables. Except feature_0 is unique by being binary.\n\n\nCleaning Data üßπ\n::: {#cell-15 .cell _kg_hide-input=‚Äòtrue‚Äô execution_count=9}\n#hide_input\nhas_nulls = set(df.columns[df.isnull().sum()!=0])\nprint(\"There are \"+str(len(has_nulls))+\" many cols with at least one null value\")\nprint(has_nulls)\n\nThere are 88 many cols with at least one null value\n{'feature_108', 'feature_91', 'feature_115', 'feature_128', 'feature_93', 'feature_33', 'feature_24', 'feature_4', 'feature_79', 'feature_28', 'feature_19', 'feature_88', 'feature_56', 'feature_117', 'feature_31', 'feature_21', 'feature_7', 'feature_94', 'feature_16', 'feature_76', 'feature_96', 'feature_12', 'feature_55', 'feature_29', 'feature_120', 'feature_35', 'feature_124', 'feature_32', 'feature_74', 'feature_17', 'feature_116', 'feature_97', 'feature_86', 'feature_105', 'feature_127', 'feature_36', 'feature_99', 'feature_34', 'feature_104', 'feature_10', 'feature_100', 'feature_58', 'feature_87', 'feature_111', 'feature_122', 'feature_80', 'feature_78', 'feature_25', 'feature_18', 'feature_59', 'feature_26', 'feature_73', 'feature_92', 'feature_15', 'feature_81', 'feature_27', 'feature_13', 'feature_112', 'feature_109', 'feature_125', 'feature_3', 'feature_98', 'feature_82', 'feature_84', 'feature_45', 'feature_90', 'feature_9', 'feature_8', 'feature_118', 'feature_75', 'feature_123', 'feature_22', 'feature_11', 'feature_23', 'feature_44', 'feature_20', 'feature_114', 'feature_106', 'feature_14', 'feature_102', 'feature_129', 'feature_110', 'feature_85', 'feature_126', 'feature_121', 'feature_30', 'feature_103', 'feature_72'}\n\n:::\nA lot of the histogram of above features has extreme outliers. For the full enlarged version of the histograms, check out here It would be safe to fill the null values with medians. Other imputation method considered were mean and KNN-Imputation. Check out my other notebook where KNN-Imputation was used to train MLP.\n::: {#cell-17 .cell _kg_hide-input=‚Äòtrue‚Äô execution_count=21}\n#hide_input\nnan_count = df.isna().sum()[df.isna().sum() &gt; 0].sort_values(ascending=False)\nprint(\"feature with most nans: feature_27 with \" + str(nan_count[0]))\nfig, axs = plt.subplots(figsize=(10, 10))\nsns.barplot(y = nan_count.index[0:30], \n            x = nan_count.values[0:30], \n            alpha = 0.8\n           )\nplt.title('Number of Nans')\nplt.xlabel('Number of Nans')\nplt.show()\n\nfeature with most nans: feature_27, with 395535\n\n\n\n\n\n\n\n\n:::\nIf we just remove all nans, we would be removing more than 16.54% of the dataset.\n::: {#cell-19 .cell _kg_hide-input=‚Äòtrue‚Äô}\n#hide_input\ndf = df.apply(lambda x: x.fillna(x.median()),axis=0)\nprint(\"Number of features with null values after median imputation: \",np.sum(df.isna().sum()&gt;0))\n:::\nInteresting points so far: - feature_0 is binary. - A lot of features seems to be normally distributed. - A lot of missing values.\n\n\nPlots & Visualization üìä\n\nresp, resp_1, resp_2, resp_3, resp_4\n::: {#cell-23 .cell _kg_hide-input=‚Äòtrue‚Äô execution_count=25}\n#hide_input\nfig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(df['resp']).cumsum()\nresp_1= pd.Series(df['resp_1']).cumsum()\nresp_2= pd.Series(df['resp_2']).cumsum()\nresp_3= pd.Series(df['resp_3']).cumsum()\nresp_4= pd.Series(df['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\")\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();\n\n\n\n\n\n\n\n:::\nWe can see that resp is closely related to resp_4 (blue and purple). Resp_1 and resp_2 also seem to be closely related but much much linear. Resp_3 seem to be in the middle, where the shape is closer to upper group but position is slightly closer to green and orange.\n\n\nWeights\nNote: weight and resp multiplied together represents a return on the trade.\n::: {#cell-27 .cell _kg_hide-input=‚Äòtrue‚Äô execution_count=11}\n#hide_input\nplt.figure(figsize = (12,5))\nax = sns.distplot(df['weight'], \n             bins=1000, \n             kde_kws={\"clip\":(0.001,1)}, \n             hist_kws={\"range\":(0.001,1)},\n             color='purple', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\nplt.xlabel(\"Histogram of non-zero weights\", size=10)\nplt.show();\ndel values\ngc.collect();\n\n\n\n\n\n\n\n:::\nWe can see that most weights are around 0.2 and we can see two ‚Äòpeaks‚Äô which is around 0.2 and 0.3. Note that maximum weight was 167.29 represented by 1.0 on x-axis. Thus 0.2 represents around 33.458 and 0.3 represents around 50.187.\n::: {#cell-29 .cell _kg_hide-input=‚Äòtrue‚Äô execution_count=9}\n#hide_input\ndf['weight_resp']   = df['weight']*df['resp']\ndf['weight_resp_1'] = df['weight']*df['resp_1']\ndf['weight_resp_2'] = df['weight']*df['resp_2']\ndf['weight_resp_3'] = df['weight']*df['resp_3']\ndf['weight_resp_4'] = df['weight']*df['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+(df.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+(df.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+(df.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+(df.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+(df.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return(500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\nplt.legend(loc=\"lower left\")\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect()\n\n2921\n\n\n\n\n\n\n\n\n:::\nNote that the graph plots all the positive gains. (Our 1‚Äôs for our action column). So we can see that there were ‚Äòbigger‚Äô gains in the beginning and as time approach 500, the gain becomes smaller. In conclusion, the earlier trades are much bigger but we don‚Äôt know what it‚Äôs going to be like in our competition test set.\n\n#hide_input\nsns.scatterplot(data=df, x='resp',y='weight', color= 'purple', alpha=.3)\nplt.title('Resp vs Weight\\ncorrelation={}'.format(round(df.weight.corr(df.resp),4)));\n\n\n\n\n\n\n\n\nWe know that we probability want to invest more ‚Äòweight‚Äô if there are bigger ‚Äòresp‚Äô(return). We learn here that higher weights are only when resp is close to 0. In other words, it is dumb to trade if resp is away from 0 but it is safe to invest even a lot if it is near 0.\n::: {#cell-33 .cell _kg_hide-input=‚Äòtrue‚Äô execution_count=13}\n#hide_input\ntrades_per_day = df.groupby(['date'])['ts_id'].count()\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(trades_per_day, color=\"purple\")\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Total number of ts_id for each day\", fontsize=18)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show()\ngc.collect()\n\n\n\n\n\n\n\n\n74963\n\n:::\nIn the Kaggle community, there‚Äôs been lots of discussion on how the trends changed significantly since day ~85. We can see much more trades happening before day 100. Rest of the days are still very active but not as noisy. We can suggest that there has been a change of trading model from Jane Street as discussed here by Carl.\nLet us look at the most important feature, ‚Äòfeature_0‚Äô\n\ndf['feature_0'].value_counts()\n\n 1    1207005\n-1    1183486\nName: feature_0, dtype: int64\n\n\n::: {#cell-37 .cell _kg_hide-input=‚Äòtrue‚Äô execution_count=15}\n#hide_input\nfeature_0_is_plus_one  = df.query('feature_0 ==  1').reset_index(drop = True)\nfeature_0_is_minus_one = df.query('feature_0 == -1').reset_index(drop = True)\n# the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return', color=\"purple\")\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return', color=\"violet\")\nax1.set_title (\"feature_0 = 1\", fontsize=18)\nax2.set_title (\"feature_0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"lower right\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();\n\n\n\n\n\n\n\n:::\nInterestingly, when feature_0 is 1, plot shows negative slope while in contrast, when feature_0 is -1, plot shows positive slope. My guess is that feature_0 corresponds to Buy(1) and Sell(-1) or vice versa. So if we set action to 1 with feature_0 = 1 then we are selling and when we set action to 0 with feature_0 = -1, then we are buying. This makes sense since whether we are buying or selling we can still lose or gain profit.\n\n\nFeatures\nRemember that we have another file called features.csv. Which can help us understand 100+ features and maybe cluster into groups. Let‚Äôs take a look. \n\n#hide_input\nfrom IPython.display import Image\n# Load image from local storage\nImage(filename = \"images/tags.png\", width =800, height = 600)\n\n\n\n\n\n\n\n\n\n#hide_input\n# fig = px.bar(feat.set_index('feature').T.sum(), title='Number of tags for each feature')\n\n# fig.layout.xaxis.tickangle = 300\n# fig.update_traces( showlegend = False)\n# fig.layout.xaxis. dtick = 5\n# fig.layout.xaxis.title = ''\n# fig.layout.yaxis.title = ''\n# fig.show()\n\nLet us see what tag_0 groups tells us.\n\n#hide_input\ncategories =  defaultdict(list)\n\nfor columns  in feat.columns[1:]:\n        categories[f'{columns}'].append(feat.query(f'{columns} == True')['feature'].to_list())\n\ncorr = df[[*categories['tag_0'][0]]].corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(45, 20))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap='BrBG',  center=0,vmin=-1, vmax=1, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n\n\n\n\n\n\n\nCorrelation between features of tag_0. It looks like there certainly are correlation between elements of the group except a few.\nInteresting points: - feature_0 has no tags - feature 79 to 119 all has 4 tags - feature 7 to 36 have 3 and 4 tags periodically - Similar trend between 2 to 7, 37 to 40, 120 to 129 - tag_n doesn‚Äôt tell too much about the features\n\n\n\nReference üìñ\n\nJane Street: EDA of day 0 and feature importance\nJane_street_Extensive_EDA & PCA starter üìä‚ö°\nEDA / A Quant‚Äôs Prespective\n\n\n\nSubmission\nIn another notebook. ### Implementation Planning Thoughts going into predicting phase. 1. Days before ~100 can be dropped as suspicion of model shift. 2. Feature_0 seem very important to find slope of cummulative resp. 3. Resp near 0 is prefered over other values. 4. A lot of features are normally distributed. 5. We have over 2 million datas, it would be safe to add lot more features(feature enginerring) 6. There are a lot of missing values too. Can try mean, median or KNN imputation methods. 7. Note that although this is kind of a time series data, we can only predict with features 0 to 129"
  },
  {
    "objectID": "posts/2021-04-04-Short-Storytime.html",
    "href": "posts/2021-04-04-Short-Storytime.html",
    "title": "Book Reviews and Storytime üìö",
    "section": "",
    "text": "Welcome to my library - toc: true - branch: master - badges: true - comments: true - author: Jaekang Lee - image: images/library.jpg - categories: [fastpages, jupyter,meme, book_review, story]\nThe simplicity and the colorful cover made me decide to buy this book. I am glad that I purchased this book because as a statistic student this book showed me the real world statistics that textbooks cannot show with their equations and proofs. My favorite story was about a killer disguised as a doctor who killed tens of innocent seniors and his suspicions were confirmed with statistical methods. Every chapter starts with a really interesting question, for example, ‚ÄúWho was the luckiest person on Titanic?‚Äù and answers with a statistical approach. It was delightful seeing concepts like p-value, hypothesis testing, regression and inference playing in real life problems, sometimes comically failing.\nI would definitely recommend this book to anyone wondering ‚ÄúI know statistics are useful but where do we see these being used?‚Äù.  Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\nThis was my first ever machine learning book that I bought because I suddenly got really interested in data science. I remember reading this book everyday and getting excited when I copied-pasted the algorithms shown in the book and they worked. The book was definetly beginner friendly in the beginning but it becomes really challenging in the later chapters. For example reading about attention mechanism in recurrent neural networks. (I am still confused to this day). The book really shines because it explains everything with pictures and diagrams. Also note they have a Git repository for the book so you can quickly use any machine learning you learn in real life.\nI would definetly recommen this book to anyone wanting to learn about machine learning. I wouldn‚Äôt rely just on the book though, it will be really helpful to take course or other similiar books as well because I find that learning about machine learning is best when you just keep reading and practicing more.  Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê\nO‚ÄôReilly always produce pretty cover that it is hard to resist not buying their book. I got interested in generative deep learning because it is one of the big machine learning pieces(generative learning, unsupervised learning, supervised learning, reinforcement learning). When I was reading this book, I really felt the author really cares about this book and that they took a lot of effort writing this book. I loved its way of explaining deep learning concepts through simple pictures and stories. For example, it compares encoders and decoders as a painter trying to fool a museum owner making him think that the painting is real when it is really fake.\nOverall, this book does not answer every question you have since generative deep learning is fairly unexplored area but if you want to quickly learn enough to get yourself started inplementing genrative deep learning models, this is a great book.  Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê\nKaggle is a great data science online community created by Google. They often hold highly competitive global data science competitions where a small data scientist like myself gain huge amount of experience, often by consuming tiny crumbles of informations and practices from experts. This book was written by one of the biggest giant in the community who have won multiple competitions. What the book does the best is guiding readers creating their own data science projects. If you ever get stuck in one area, the book gives you clear directions. If you are a programmer interested in writing machine learning applications, this is a great guide.  Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n#hide_input\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n#hide_input\nmonty = PILImage.create(\"story/monty.jpeg\")\ndisplay(monty.to_thumb(400,400))\nIn this 1963 American tv show, the host Monty Hall proposes a game to the contestant. There are three closed doors. Two of the doors have goats behind them and one of the doors has a fancy car behind it. The contestants get to keep whatever is behind the door they choose. Once the contestant chooses one of the doors, Monty opens one of the unchosen doors showing a goat behind it. Then Monty asks the contestant if he/she wants to change their decision on the remaining two doors. Should the contestant change?\nMost people will think the contestant has 1/2 chance of winning. I too thought this way but by switching, the contestant actually doubles their chance of winning. To see this most clearly, consider a similar test with 100 doors instead of 3. Now, once you choose a door, Monty opens 98 other doors showing goats and asks you if you want to switch. Obviously, you should switch to the other door because the original choice had 1/100 chance of winning but the other door is same as having had 99 choices to choose the correct door with 99% chance of winning. So in our original case, the door we chose initially has 1/3 chance of winning but choosing the other door has 2/3 chance. Hence doubling our chance of winning. If you are still not convinced, you can try out yourself here http://www.shodor.org/interactivate/activities/SimpleMontyHall/."
  },
  {
    "objectID": "posts/2021-04-04-Short-Storytime.html#reference",
    "href": "posts/2021-04-04-Short-Storytime.html#reference",
    "title": "Book Reviews and Storytime üìö",
    "section": "Reference",
    "text": "Reference\nBooks: - Naked Statistics: Stripping the Dread from the Data Book(2012) by Charles Wheelan"
  },
  {
    "objectID": "posts/2021-05-02-nmu.html",
    "href": "posts/2021-05-02-nmu.html",
    "title": "Non-medical Drug Use in Canada üçÅ",
    "section": "",
    "text": "DataFest 2021 @ University of Toronto - toc: true - branch: master - badges: true - comments: true - author: Jaekang Lee - categories: [fastpages, python, glmer, data_exploration, data wrangling, report, PCA, mutual_info, drug]\nFull project - link"
  },
  {
    "objectID": "posts/2021-05-02-nmu.html#introduction-background",
    "href": "posts/2021-05-02-nmu.html#introduction-background",
    "title": "Non-medical Drug Use in Canada üçÅ",
    "section": "Introduction & Background",
    "text": "Introduction & Background\nThe growing non-medical use of prescription drugs is a global health concern. The Canadian‚Äôs Non-Medical Use of Prescription Drugs Survey data collected by RADARS¬Æ System gives an opportunity to understand the situation. Our team explored the raw survey data by visualizing, and came up with three research questions for the report.\n\nDataset\nhttps://github.com/leejaeka/Datafest2021/blob/main/data/CA-Data/Data%20Dictionary%20CA%2017Q3.doc\n\n\nData Wrangling/ Feature Engineering\n\nAssumptions\n\nWe assumed all surveys were answered truthfully and recorded without errors\n\n\n\nQuestions/Problems\n\nThere are 185 variables in total to consider. How do we explore all of these?\nAbout 20% of the cells in the dataset are NANs.\n\n\n\nSolution\n\nWe used principal components and mutual information to compute which variables to keep.\nWe fill the missing value with best logical answers. For example, variables on pregnancy for all males were ‚ÄòNA‚Äô. We fill the missing value with 0.\n\n\n\n\nResearch Question 1: What types of social groups best explain non-medical use?\nWe used principal components and mutual information to compute which variables to keep. #### Steps 1. Principal components algorithm to reduce dimensionality and capture social groups. 2. Took the top components from PCA then calculated mutual information to find which component best explained ‚ÄòNMU‚Äô variable 3. Visualized each social groups\n\n#hide_input\nfrom fastbook import *\nfrom fastai.vision.widgets import *\none = PILImage.create(\"images/datafest/PCA_MI.png\")\n\ndisplay(one)\n#display(one.to_thumb(800,700))\n\nC:\\Users\\tonyl\\anaconda3\\lib\\site-packages\\fastbook\\__init__.py:18: UserWarning: Missing `graphviz` - please run `conda install fastbook`\n  except ModuleNotFoundError: warn(\"Missing `graphviz` - please run `conda install fastbook`\")\n\n\n\n\n\n\n\n\n\nAbove graph shows the process of capturing ‚ÄòTop Non-Medical Drug Use Social Groups‚Äô. - Top Mutual Information of ‚ÄòNMU‚Äô: MI scores of original dataset with ‚ÄòNMU‚Äô as response variable. Showed that Opioid, Codeine and Coccaine were the top explanation for NMU. - Principal Component Variance Capture Plots: We can see that PCA succesfully captured about 90% of total variance with just 100 components. - Top Mutual Information of ‚ÄòNMU‚Äô with Principal Components: Greatest score was ‚ÄòPC3‚Äô followed by ‚ÄòPC10‚Äô then so on. Below it, we can see the example of dissecting PC3 where it is used to find out that PC3 corresponds to people from Quebec.\nView interactive visualization_1 in Tableau - link\n\n\nTop Non-Medical Drug Use Social Groups\n\n#hide_input\ntwo = PILImage.create(\"images/datafest/MAPS.png\")\ndisplay(two)\n\n\n\n\n\n\n\n\nView interactive visualization_2 in Tableau - link\n\n#hide_input\nthree = PILImage.create(\"images/datafest/GROUPS.png\")\ndisplay(three)\n\n\n\n\n\n\n\n\nThe specificness of PCA‚Äôs component interpretation allows us to analyze the group closely. (Without having to explore billions of possible combinations of groups)\n\n#hide_input\nfour = PILImage.create(\"images/datafest/specific.png\")\ndisplay(four)\n\n\n\n\n\n\n\n\nView interactive visualization_3 in Tableau - link\n\n#hide_input\nfive = PILImage.create(\"images/datafest/slide.png\")\ndisplay(five)\n\n\n\n\n\n\n\n\n\n\nConclusion\n\nQuebec was most severe social group suffering from Non-Medical Drug Use in Canada and we have looked at several other social groups\n\n\n#hide_input\nsix = PILImage.create(\"images/datafest/conclusion.png\")\ndisplay(six)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]